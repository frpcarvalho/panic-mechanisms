{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59773911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üî¨ PAPER 3 - PHASE 1.1: DISTRIBUTIONAL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìÇ Output directory: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/phase1_distributions\n",
      "\n",
      "üìä Loading data...\n",
      "   ‚úÖ Loaded 6581 samples with 13 columns\n",
      "\n",
      "‚ö†Ô∏è  Missing values detected:\n",
      "   BIX_BIDFAT: 1901 (28.89%)\n",
      "   DEMO_INDFMPIR: 423 (6.43%)\n",
      "   BPX_BPXDAR: 429 (6.52%)\n",
      "   WHQ_WHD050: 4 (0.06%)\n",
      "   BMX_BMXBMI: 233 (3.54%)\n",
      "   ALQ_ALQ130: 1917 (29.13%)\n",
      "   DEMO_DMDMARTL: 176 (2.67%)\n",
      "\n",
      "   Strategy: Will drop rows with any missing values for clean analysis\n",
      "   ‚úÖ Clean dataset: 3279 samples (3302 dropped)\n",
      "\n",
      "üéØ Sample sizes:\n",
      "   Panic Disorder: 115 (3.51%)\n",
      "   Normal:         3164 (96.49%)\n",
      "\n",
      "================================================================================\n",
      "üìà COMPUTING STATISTICAL METRICS\n",
      "================================================================================\n",
      "\n",
      "üîç Analyzing: BIX_BIDFAT\n",
      "   PD Mean: 26.254 ¬± 13.685\n",
      "   Normal Mean: 22.911 ¬± 11.037\n",
      "   Cohen's d: 0.300 (Small)\n",
      "   Overlap: 0.858 (14.2% separation)\n",
      "   T-test p-value: 1.59e-03\n",
      "   KS-test p-value: 2.51e-11\n",
      "\n",
      "üîç Analyzing: DEMO_INDFMPIR\n",
      "   PD Mean: 2.063 ¬± 1.474\n",
      "   Normal Mean: 2.913 ¬± 1.608\n",
      "   Cohen's d: -0.531 (Medium)\n",
      "   Overlap: 0.780 (22.0% separation)\n",
      "   T-test p-value: 2.46e-08\n",
      "   KS-test p-value: 1.32e-06\n",
      "\n",
      "üîç Analyzing: DEMO_RIDAGEMN\n",
      "   PD Mean: 367.096 ¬± 58.069\n",
      "   Normal Mean: 353.745 ¬± 70.208\n",
      "   Cohen's d: 0.191 (Negligible)\n",
      "   Overlap: 0.882 (11.8% separation)\n",
      "   T-test p-value: 4.41e-02\n",
      "   KS-test p-value: 7.32e-07\n",
      "\n",
      "üîç Analyzing: DEMO_RIAGENDR\n",
      "   PD Mean: 1.800 ¬± 0.402\n",
      "   Normal Mean: 1.454 ¬± 0.498\n",
      "   Cohen's d: 0.700 (Medium)\n",
      "   Overlap: 0.690 (31.0% separation)\n",
      "   T-test p-value: 2.08e-13\n",
      "   KS-test p-value: 2.14e-12\n",
      "\n",
      "üîç Analyzing: BPX_BPXDAR\n",
      "   PD Mean: 68.852 ¬± 5.612\n",
      "   Normal Mean: 69.417 ¬± 11.539\n",
      "   Cohen's d: -0.050 (Negligible)\n",
      "   Overlap: 0.665 (33.5% separation)\n",
      "   T-test p-value: 6.01e-01\n",
      "   KS-test p-value: 2.84e-07\n",
      "\n",
      "üîç Analyzing: WHQ_WHD050\n",
      "   PD Mean: 152.696 ¬± 33.357\n",
      "   Normal Mean: 574.006 ¬± 6155.641\n",
      "   Cohen's d: -0.070 (Negligible)\n",
      "   Overlap: 0.015 (98.5% separation)\n",
      "   T-test p-value: 4.63e-01\n",
      "   KS-test p-value: 2.10e-07\n",
      "\n",
      "üîç Analyzing: DEMO_DMDHHSIZ\n",
      "   PD Mean: 3.052 ¬± 1.323\n",
      "   Normal Mean: 3.236 ¬± 1.567\n",
      "   Cohen's d: -0.118 (Negligible)\n",
      "   Overlap: 0.907 (9.3% separation)\n",
      "   T-test p-value: 2.15e-01\n",
      "   KS-test p-value: 9.64e-03\n",
      "\n",
      "üîç Analyzing: BMX_BMXBMI\n",
      "   PD Mean: 27.262 ¬± 5.493\n",
      "   Normal Mean: 26.769 ¬± 5.785\n",
      "   Cohen's d: 0.085 (Negligible)\n",
      "   Overlap: 0.959 (4.1% separation)\n",
      "   T-test p-value: 3.69e-01\n",
      "   KS-test p-value: 2.08e-04\n",
      "\n",
      "üîç Analyzing: ALQ_ALQ130\n",
      "   PD Mean: 3.261 ¬± 1.992\n",
      "   Normal Mean: 3.259 ¬± 3.959\n",
      "   Cohen's d: 0.000 (Negligible)\n",
      "   Overlap: 0.680 (32.0% separation)\n",
      "   T-test p-value: 9.97e-01\n",
      "   KS-test p-value: 7.47e-03\n",
      "\n",
      "üîç Analyzing: MCQ_MCQ250F\n",
      "   PD Mean: 1.330 ¬± 0.472\n",
      "   Normal Mean: 1.607 ¬± 0.489\n",
      "   Cohen's d: -0.566 (Medium)\n",
      "   Overlap: 0.773 (22.7% separation)\n",
      "   T-test p-value: 2.68e-09\n",
      "   KS-test p-value: 5.54e-08\n",
      "\n",
      "üîç Analyzing: MPQ_MPQ070\n",
      "   PD Mean: 1.365 ¬± 0.484\n",
      "   Normal Mean: 1.635 ¬± 0.482\n",
      "   Cohen's d: -0.559 (Medium)\n",
      "   Overlap: 0.780 (22.0% separation)\n",
      "   T-test p-value: 4.20e-09\n",
      "   KS-test p-value: 1.33e-07\n",
      "\n",
      "üîç Analyzing: DEMO_DMDMARTL\n",
      "   PD Mean: 2.887 ¬± 1.621\n",
      "   Normal Mean: 3.349 ¬± 2.002\n",
      "   Cohen's d: -0.232 (Small)\n",
      "   Overlap: 0.864 (13.6% separation)\n",
      "   T-test p-value: 1.46e-02\n",
      "   KS-test p-value: 1.23e-06\n",
      "\n",
      "================================================================================\n",
      "üíæ SAVING STATISTICAL RESULTS\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Full results saved to: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/phase1_distributions/statistical_analysis_complete.csv\n",
      "‚úÖ Summary table saved to: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/phase1_distributions/Table1_descriptive_statistics.csv\n",
      "\n",
      "================================================================================\n",
      "üé® CREATING VISUALIZATIONS\n",
      "================================================================================\n",
      "\n",
      "üìä Creating comprehensive violin plots...\n",
      "‚úÖ Violin plots saved to: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/phase1_distributions/Figure1_violin_plots_all_features.png\n",
      "\n",
      "üìä Creating effect sizes ranking plot...\n",
      "‚úÖ Effect sizes plot saved to: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/phase1_distributions/Figure2_effect_sizes_ranked.png\n",
      "\n",
      "üìä Creating separation heatmap...\n",
      "‚úÖ Separation heatmap saved to: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/phase1_distributions/Figure3_separation_heatmap.png\n",
      "\n",
      "üìä Creating overlapping distribution plots (top 4 features by effect size)...\n",
      "‚úÖ Distribution overlaps saved to: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/phase1_distributions/Figure4_distribution_overlaps_top4.png\n",
      "\n",
      "================================================================================\n",
      "üß¨ BIOLOGICAL DOMAIN ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üîç Analyzing domain: Biological\n",
      "   Features: 4\n",
      "   Mean |Cohen's d|: 0.126\n",
      "   Max |Cohen's d|: 0.300\n",
      "   Features: ['Diastolic Blood Pressure', 'Body Mass Index', 'Body Fat Mass (kg)', 'Weight 1 Year Ago']\n",
      "\n",
      "üîç Analyzing domain: Psychological\n",
      "   Features: 2\n",
      "   Mean |Cohen's d|: 0.563\n",
      "   Max |Cohen's d|: 0.566\n",
      "   Features: ['Lower Back Pain', 'Family History HTN/Stroke']\n",
      "\n",
      "üîç Analyzing domain: Social\n",
      "   Features: 6\n",
      "   Mean |Cohen's d|: 0.295\n",
      "   Max |Cohen's d|: 0.700\n",
      "   Features: ['Poverty Income Ratio', 'Marital Status', 'Gender', 'Age (months)', 'Household Size', 'Alcohol Consumption']\n",
      "\n",
      "‚úÖ Domain analysis saved to: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/phase1_distributions/Table2_domain_analysis.csv\n",
      "‚úÖ Domain comparison plot saved to: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/phase1_distributions/Figure5_domain_comparison.png\n",
      "\n",
      "================================================================================\n",
      "üìù GENERATING SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Summary report saved to: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/phase1_distributions/PHASE1_SUMMARY_REPORT.txt\n",
      "\n",
      "================================================================================\n",
      "‚úÖ PHASE 1.1 COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üìÇ All results saved to: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/phase1_distributions\n",
      "\n",
      "üìä Generated outputs:\n",
      "   - statistical_analysis_complete.csv\n",
      "   - Table1_descriptive_statistics.csv\n",
      "   - Table2_domain_analysis.csv\n",
      "   - Figure1_violin_plots_all_features.png\n",
      "   - Figure2_effect_sizes_ranked.png\n",
      "   - Figure3_separation_heatmap.png\n",
      "   - Figure4_distribution_overlaps_top4.png\n",
      "   - Figure5_domain_comparison.png\n",
      "   - PHASE1_SUMMARY_REPORT.txt\n",
      "\n",
      "üöÄ Ready for Phase 1.2: Multidimensional Visualization!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PAPER 3 - PHASE 1.1: DISTRIBUTIONAL ANALYSIS\n",
    "============================================\n",
    "Deep dive into feature distributions to understand how each variable\n",
    "discriminates between Panic Disorder and Normal cases.\n",
    "\n",
    "Author: Generated for Panic Disorder ML Investigation\n",
    "Date: 2025\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = '/Users/filipecarvalho/Documents/data_science_projects/Panic.3/NHANES_panic_12features.csv'\n",
    "OUTPUT_DIR = Path('/Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/phase1_distributions')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Feature names with descriptions\n",
    "FEATURE_INFO = {\n",
    "    'BIX_BIDFAT': 'Body Fat Mass (kg)',\n",
    "    'DEMO_INDFMPIR': 'Poverty Income Ratio',\n",
    "    'DEMO_RIDAGEMN': 'Age (months)',\n",
    "    'DEMO_RIAGENDR': 'Gender',\n",
    "    'BPX_BPXDAR': 'Diastolic Blood Pressure',\n",
    "    'WHQ_WHD050': 'Weight 1 Year Ago',\n",
    "    'DEMO_DMDHHSIZ': 'Household Size',\n",
    "    'BMX_BMXBMI': 'Body Mass Index',\n",
    "    'ALQ_ALQ130': 'Alcohol Consumption',\n",
    "    'MCQ_MCQ250F': 'Family History HTN/Stroke',\n",
    "    'MPQ_MPQ070': 'Lower Back Pain',\n",
    "    'DEMO_DMDMARTL': 'Marital Status'\n",
    "}\n",
    "\n",
    "# Biological grouping for theoretical analysis\n",
    "BIOLOGICAL_DOMAINS = {\n",
    "    'Biological': ['BPX_BPXDAR', 'BMX_BMXBMI', 'BIX_BIDFAT', 'WHQ_WHD050'],\n",
    "    'Psychological': ['MPQ_MPQ070', 'MCQ_MCQ250F'],\n",
    "    'Social': ['DEMO_INDFMPIR', 'DEMO_DMDMARTL', 'DEMO_RIAGENDR', \n",
    "               'DEMO_RIDAGEMN', 'DEMO_DMDHHSIZ', 'ALQ_ALQ130']\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üî¨ PAPER 3 - PHASE 1.1: DISTRIBUTIONAL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìÇ Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# ==================== LOAD DATA ====================\n",
    "print(\"\\nüìä Loading data...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"   ‚úÖ Loaded {len(df)} samples with {df.shape[1]} columns\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_summary = df.isnull().sum()\n",
    "if missing_summary.sum() > 0:\n",
    "    print(\"\\n‚ö†Ô∏è  Missing values detected:\")\n",
    "    for col in missing_summary[missing_summary > 0].index:\n",
    "        pct = (missing_summary[col] / len(df)) * 100\n",
    "        print(f\"   {col}: {missing_summary[col]} ({pct:.2f}%)\")\n",
    "    print(\"\\n   Strategy: Will drop rows with any missing values for clean analysis\")\n",
    "    df_clean = df.dropna()\n",
    "    print(f\"   ‚úÖ Clean dataset: {len(df_clean)} samples ({len(df)-len(df_clean)} dropped)\")\n",
    "else:\n",
    "    df_clean = df.copy()\n",
    "    print(\"   ‚úÖ No missing values!\")\n",
    "\n",
    "# Separate by group\n",
    "df_pd = df_clean[df_clean['target'] == 1]\n",
    "df_normal = df_clean[df_clean['target'] == 0]\n",
    "\n",
    "print(f\"\\nüéØ Sample sizes:\")\n",
    "print(f\"   Panic Disorder: {len(df_pd)} ({len(df_pd)/len(df_clean)*100:.2f}%)\")\n",
    "print(f\"   Normal:         {len(df_normal)} ({len(df_normal)/len(df_clean)*100:.2f}%)\")\n",
    "\n",
    "features = list(FEATURE_INFO.keys())\n",
    "\n",
    "# ==================== STATISTICAL ANALYSIS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìà COMPUTING STATISTICAL METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "\n",
    "for feat in features:\n",
    "    print(f\"\\nüîç Analyzing: {feat}\")\n",
    "    \n",
    "    # Get data\n",
    "    pd_vals = df_pd[feat].values\n",
    "    normal_vals = df_normal[feat].values\n",
    "    \n",
    "    # Basic statistics\n",
    "    pd_mean = np.mean(pd_vals)\n",
    "    pd_std = np.std(pd_vals, ddof=1)\n",
    "    pd_median = np.median(pd_vals)\n",
    "    pd_min = np.min(pd_vals)\n",
    "    pd_max = np.max(pd_vals)\n",
    "    \n",
    "    normal_mean = np.mean(normal_vals)\n",
    "    normal_std = np.std(normal_vals, ddof=1)\n",
    "    normal_median = np.median(normal_vals)\n",
    "    normal_min = np.min(normal_vals)\n",
    "    normal_max = np.max(normal_vals)\n",
    "    \n",
    "    # T-test\n",
    "    t_stat, t_pval = stats.ttest_ind(pd_vals, normal_vals)\n",
    "    \n",
    "    # Kolmogorov-Smirnov test\n",
    "    ks_stat, ks_pval = stats.ks_2samp(pd_vals, normal_vals)\n",
    "    \n",
    "    # Cohen's d (effect size)\n",
    "    pooled_std = np.sqrt(((len(pd_vals)-1)*pd_std**2 + (len(normal_vals)-1)*normal_std**2) / \n",
    "                         (len(pd_vals) + len(normal_vals) - 2))\n",
    "    cohens_d = (pd_mean - normal_mean) / pooled_std\n",
    "    \n",
    "    # Effect size interpretation\n",
    "    if abs(cohens_d) < 0.2:\n",
    "        effect_size_interp = \"Negligible\"\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        effect_size_interp = \"Small\"\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        effect_size_interp = \"Medium\"\n",
    "    else:\n",
    "        effect_size_interp = \"Large\"\n",
    "    \n",
    "    # Overlap coefficient (approximate using normal distributions)\n",
    "    # Area under the minimum of two normal distributions\n",
    "    def overlap_coefficient(mu1, sigma1, n1, mu2, sigma2, n2):\n",
    "        \"\"\"Calculate overlap coefficient between two distributions\"\"\"\n",
    "        # Create range for integration\n",
    "        x_min = min(mu1 - 4*sigma1, mu2 - 4*sigma2)\n",
    "        x_max = max(mu1 + 4*sigma1, mu2 + 4*sigma2)\n",
    "        x = np.linspace(x_min, x_max, 10000)\n",
    "        \n",
    "        # PDF for both distributions\n",
    "        pdf1 = stats.norm.pdf(x, mu1, sigma1)\n",
    "        pdf2 = stats.norm.pdf(x, mu2, sigma2)\n",
    "        \n",
    "        # Overlap is integral of minimum\n",
    "        overlap = np.trapz(np.minimum(pdf1, pdf2), x)\n",
    "        return overlap\n",
    "    \n",
    "    overlap = overlap_coefficient(pd_mean, pd_std, len(pd_vals), \n",
    "                                  normal_mean, normal_std, len(normal_vals))\n",
    "    \n",
    "    # Mann-Whitney U test (non-parametric alternative)\n",
    "    u_stat, u_pval = stats.mannwhitneyu(pd_vals, normal_vals, alternative='two-sided')\n",
    "    \n",
    "    print(f\"   PD Mean: {pd_mean:.3f} ¬± {pd_std:.3f}\")\n",
    "    print(f\"   Normal Mean: {normal_mean:.3f} ¬± {normal_std:.3f}\")\n",
    "    print(f\"   Cohen's d: {cohens_d:.3f} ({effect_size_interp})\")\n",
    "    print(f\"   Overlap: {overlap:.3f} ({(1-overlap)*100:.1f}% separation)\")\n",
    "    print(f\"   T-test p-value: {t_pval:.2e}\")\n",
    "    print(f\"   KS-test p-value: {ks_pval:.2e}\")\n",
    "    \n",
    "    results.append({\n",
    "        'Feature': feat,\n",
    "        'Description': FEATURE_INFO[feat],\n",
    "        'PD_Mean': pd_mean,\n",
    "        'PD_Std': pd_std,\n",
    "        'PD_Median': pd_median,\n",
    "        'PD_Min': pd_min,\n",
    "        'PD_Max': pd_max,\n",
    "        'Normal_Mean': normal_mean,\n",
    "        'Normal_Std': normal_std,\n",
    "        'Normal_Median': normal_median,\n",
    "        'Normal_Min': normal_min,\n",
    "        'Normal_Max': normal_max,\n",
    "        'Mean_Diff': pd_mean - normal_mean,\n",
    "        'Cohens_d': cohens_d,\n",
    "        'Effect_Size': effect_size_interp,\n",
    "        'Overlap_Coef': overlap,\n",
    "        'Separation_Pct': (1 - overlap) * 100,\n",
    "        'T_Statistic': t_stat,\n",
    "        'T_Pvalue': t_pval,\n",
    "        'KS_Statistic': ks_stat,\n",
    "        'KS_Pvalue': ks_pval,\n",
    "        'U_Statistic': u_stat,\n",
    "        'U_Pvalue': u_pval\n",
    "    })\n",
    "\n",
    "# Create results DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Add absolute value column for sorting and analysis\n",
    "df_results['Cohens_d_abs'] = df_results['Cohens_d'].abs()\n",
    "\n",
    "# ==================== SAVE STATISTICAL RESULTS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ SAVING STATISTICAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Full results table\n",
    "output_file = OUTPUT_DIR / 'statistical_analysis_complete.csv'\n",
    "df_results.to_csv(output_file, index=False)\n",
    "print(f\"\\n‚úÖ Full results saved to: {output_file}\")\n",
    "\n",
    "# Summary table (for paper)\n",
    "summary_cols = ['Feature', 'Description', 'PD_Mean', 'PD_Std', 'Normal_Mean', \n",
    "                'Normal_Std', 'Cohens_d', 'Effect_Size', 'Separation_Pct', 'T_Pvalue']\n",
    "df_summary = df_results[summary_cols].copy()\n",
    "df_summary = df_summary.round({\n",
    "    'PD_Mean': 2, 'PD_Std': 2, 'Normal_Mean': 2, 'Normal_Std': 2,\n",
    "    'Cohens_d': 3, 'Separation_Pct': 1\n",
    "})\n",
    "df_summary['T_Pvalue'] = df_summary['T_Pvalue'].apply(lambda x: f'{x:.2e}')\n",
    "\n",
    "output_file_summary = OUTPUT_DIR / 'Table1_descriptive_statistics.csv'\n",
    "df_summary.to_csv(output_file_summary, index=False)\n",
    "print(f\"‚úÖ Summary table saved to: {output_file_summary}\")\n",
    "\n",
    "# ==================== VISUALIZATIONS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üé® CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. VIOLIN PLOTS - All features in one figure\n",
    "print(\"\\nüìä Creating comprehensive violin plots...\")\n",
    "fig, axes = plt.subplots(4, 3, figsize=(20, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feat in enumerate(features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Prepare data for violin plot\n",
    "    plot_data = pd.DataFrame({\n",
    "        'Value': np.concatenate([df_pd[feat].values, df_normal[feat].values]),\n",
    "        'Group': ['Panic Disorder']*len(df_pd) + ['Normal']*len(df_normal)\n",
    "    })\n",
    "    \n",
    "    # Create violin plot\n",
    "    parts = ax.violinplot([df_pd[feat].values, df_normal[feat].values],\n",
    "                          positions=[0, 1],\n",
    "                          showmeans=True,\n",
    "                          showextrema=True,\n",
    "                          showmedians=True)\n",
    "    \n",
    "    # Color the violins\n",
    "    colors = ['#FF6B6B', '#4ECDC4']  # Red for PD, Teal for Normal\n",
    "    for pc, color in zip(parts['bodies'], colors):\n",
    "        pc.set_facecolor(color)\n",
    "        pc.set_alpha(0.7)\n",
    "    \n",
    "    # Add individual points with jitter (sample if too many)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # PD points\n",
    "    pd_sample = df_pd[feat].values\n",
    "    if len(pd_sample) > 100:\n",
    "        pd_sample = np.random.choice(pd_sample, 100, replace=False)\n",
    "    jitter_pd = np.random.normal(0, 0.04, size=len(pd_sample))\n",
    "    ax.scatter(jitter_pd, pd_sample, alpha=0.3, s=20, color='#FF6B6B', edgecolors='black', linewidths=0.5)\n",
    "    \n",
    "    # Normal points (sample more heavily)\n",
    "    normal_sample = df_normal[feat].values\n",
    "    if len(normal_sample) > 200:\n",
    "        normal_sample = np.random.choice(normal_sample, 200, replace=False)\n",
    "    jitter_normal = np.random.normal(1, 0.04, size=len(normal_sample))\n",
    "    ax.scatter(jitter_normal, normal_sample, alpha=0.2, s=10, color='#4ECDC4', edgecolors='black', linewidths=0.3)\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels(['PD', 'Normal'])\n",
    "    ax.set_title(f'{FEATURE_INFO[feat]}\\n(Cohen\\'s d = {df_results.iloc[idx][\"Cohens_d\"]:.3f})', \n",
    "                 fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Value', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add separation info\n",
    "    separation = df_results.iloc[idx]['Separation_Pct']\n",
    "    effect = df_results.iloc[idx]['Effect_Size']\n",
    "    ax.text(0.5, ax.get_ylim()[1]*0.95, f'{separation:.1f}% sep. ({effect})',\n",
    "            ha='center', va='top', fontsize=9, \n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.suptitle('Feature Distributions: Panic Disorder vs Normal\\n(Phase 1.1 - Distributional Analysis)', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "output_fig = OUTPUT_DIR / 'Figure1_violin_plots_all_features.png'\n",
    "plt.savefig(output_fig, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Violin plots saved to: {output_fig}\")\n",
    "plt.close()\n",
    "\n",
    "# 2. EFFECT SIZES RANKED\n",
    "print(\"\\nüìä Creating effect sizes ranking plot...\")\n",
    "df_sorted = df_results.sort_values('Cohens_d_abs', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "colors_bar = ['#FF6B6B' if x > 0 else '#4ECDC4' for x in df_sorted['Cohens_d']]\n",
    "bars = ax.barh(range(len(df_sorted)), df_sorted['Cohens_d'], color=colors_bar, alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Add value labels\n",
    "for i, (val, effect) in enumerate(zip(df_sorted['Cohens_d'], df_sorted['Effect_Size'])):\n",
    "    ax.text(val + 0.02 if val > 0 else val - 0.02, i, f'{val:.3f} ({effect})', \n",
    "            va='center', ha='left' if val > 0 else 'right', fontsize=9)\n",
    "\n",
    "ax.set_yticks(range(len(df_sorted)))\n",
    "ax.set_yticklabels([FEATURE_INFO[feat] for feat in df_sorted['Feature']], fontsize=10)\n",
    "ax.set_xlabel(\"Cohen's d (Effect Size)\", fontsize=12, fontweight='bold')\n",
    "ax.set_title(\"Effect Sizes: Panic Disorder vs Normal\\n(Sorted by Magnitude)\", \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.axvline(0, color='black', linewidth=1, linestyle='--')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add reference lines for effect size interpretation\n",
    "ax.axvline(0.2, color='green', linewidth=0.5, linestyle=':', alpha=0.5, label='Small (0.2)')\n",
    "ax.axvline(0.5, color='orange', linewidth=0.5, linestyle=':', alpha=0.5, label='Medium (0.5)')\n",
    "ax.axvline(0.8, color='red', linewidth=0.5, linestyle=':', alpha=0.5, label='Large (0.8)')\n",
    "ax.axvline(-0.2, color='green', linewidth=0.5, linestyle=':', alpha=0.5)\n",
    "ax.axvline(-0.5, color='orange', linewidth=0.5, linestyle=':', alpha=0.5)\n",
    "ax.axvline(-0.8, color='red', linewidth=0.5, linestyle=':', alpha=0.5)\n",
    "ax.legend(loc='lower right', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "output_fig = OUTPUT_DIR / 'Figure2_effect_sizes_ranked.png'\n",
    "plt.savefig(output_fig, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Effect sizes plot saved to: {output_fig}\")\n",
    "plt.close()\n",
    "\n",
    "# 3. SEPARATION HEATMAP\n",
    "print(\"\\nüìä Creating separation heatmap...\")\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Prepare data for heatmap\n",
    "heatmap_data = df_results.set_index('Feature')[['Separation_Pct']].T\n",
    "heatmap_data.index = ['% Separation']\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.1f', cmap='RdYlGn', \n",
    "            cbar_kws={'label': '% Separation'}, ax=ax, \n",
    "            vmin=0, vmax=100, linewidths=1, linecolor='black')\n",
    "\n",
    "ax.set_xticklabels([FEATURE_INFO[feat] for feat in heatmap_data.columns], rotation=45, ha='right')\n",
    "ax.set_title('Feature Separation Power: Panic Disorder vs Normal\\n(Higher = Better Discrimination)', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "output_fig = OUTPUT_DIR / 'Figure3_separation_heatmap.png'\n",
    "plt.savefig(output_fig, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Separation heatmap saved to: {output_fig}\")\n",
    "plt.close()\n",
    "\n",
    "# 4. OVERLAPPING DISTRIBUTIONS (Top 4 features)\n",
    "print(\"\\nüìä Creating overlapping distribution plots (top 4 features by effect size)...\")\n",
    "top4_features = df_results.nlargest(4, 'Cohens_d_abs')['Feature'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feat in enumerate(top4_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Create histograms with KDE\n",
    "    ax.hist(df_normal[feat], bins=50, alpha=0.5, color='#4ECDC4', label='Normal', density=True, edgecolor='black')\n",
    "    ax.hist(df_pd[feat], bins=30, alpha=0.5, color='#FF6B6B', label='Panic Disorder', density=True, edgecolor='black')\n",
    "    \n",
    "    # Add KDE\n",
    "    from scipy.stats import gaussian_kde\n",
    "    \n",
    "    # Normal KDE\n",
    "    kde_normal = gaussian_kde(df_normal[feat].values)\n",
    "    x_normal = np.linspace(df_normal[feat].min(), df_normal[feat].max(), 1000)\n",
    "    ax.plot(x_normal, kde_normal(x_normal), color='#4ECDC4', linewidth=2.5, label='Normal KDE')\n",
    "    \n",
    "    # PD KDE\n",
    "    kde_pd = gaussian_kde(df_pd[feat].values)\n",
    "    x_pd = np.linspace(df_pd[feat].min(), df_pd[feat].max(), 1000)\n",
    "    ax.plot(x_pd, kde_pd(x_pd), color='#FF6B6B', linewidth=2.5, label='PD KDE')\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_xlabel('Value', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    feat_info = df_results[df_results['Feature'] == feat].iloc[0]\n",
    "    ax.set_title(f'{FEATURE_INFO[feat]}\\nCohen\\'s d = {feat_info[\"Cohens_d\"]:.3f}, Overlap = {feat_info[\"Overlap_Coef\"]:.3f}',\n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Distribution Overlap Analysis: Top 4 Features by Effect Size\\n(Phase 1.1)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "output_fig = OUTPUT_DIR / 'Figure4_distribution_overlaps_top4.png'\n",
    "plt.savefig(output_fig, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Distribution overlaps saved to: {output_fig}\")\n",
    "plt.close()\n",
    "\n",
    "# ==================== DOMAIN ANALYSIS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üß¨ BIOLOGICAL DOMAIN ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "domain_results = []\n",
    "\n",
    "for domain_name, domain_features in BIOLOGICAL_DOMAINS.items():\n",
    "    print(f\"\\nüîç Analyzing domain: {domain_name}\")\n",
    "    \n",
    "    # Get effect sizes for features in this domain\n",
    "    domain_effects = df_results[df_results['Feature'].isin(domain_features)]['Cohens_d'].abs()\n",
    "    \n",
    "    print(f\"   Features: {len(domain_features)}\")\n",
    "    print(f\"   Mean |Cohen's d|: {domain_effects.mean():.3f}\")\n",
    "    print(f\"   Max |Cohen's d|: {domain_effects.max():.3f}\")\n",
    "    print(f\"   Features: {[FEATURE_INFO[f] for f in domain_features]}\")\n",
    "    \n",
    "    domain_results.append({\n",
    "        'Domain': domain_name,\n",
    "        'N_Features': len(domain_features),\n",
    "        'Mean_Effect_Size': domain_effects.mean(),\n",
    "        'Max_Effect_Size': domain_effects.max(),\n",
    "        'Min_Effect_Size': domain_effects.min(),\n",
    "        'Features': ', '.join([FEATURE_INFO[f] for f in domain_features])\n",
    "    })\n",
    "\n",
    "df_domains = pd.DataFrame(domain_results)\n",
    "output_file_domains = OUTPUT_DIR / 'Table2_domain_analysis.csv'\n",
    "df_domains.to_csv(output_file_domains, index=False)\n",
    "print(f\"\\n‚úÖ Domain analysis saved to: {output_file_domains}\")\n",
    "\n",
    "# Domain comparison plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(BIOLOGICAL_DOMAINS))\n",
    "means = [df_domains[df_domains['Domain'] == d]['Mean_Effect_Size'].values[0] for d in BIOLOGICAL_DOMAINS.keys()]\n",
    "maxs = [df_domains[df_domains['Domain'] == d]['Max_Effect_Size'].values[0] for d in BIOLOGICAL_DOMAINS.keys()]\n",
    "\n",
    "width = 0.35\n",
    "bars1 = ax.bar(x - width/2, means, width, label='Mean |Cohen\\'s d|', color='#4ECDC4', edgecolor='black')\n",
    "bars2 = ax.bar(x + width/2, maxs, width, label='Max |Cohen\\'s d|', color='#FF6B6B', edgecolor='black')\n",
    "\n",
    "ax.set_ylabel('Effect Size (|Cohen\\'s d|)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Feature Discrimination by Biological Domain', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(BIOLOGICAL_DOMAINS.keys(), fontsize=11)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "output_fig = OUTPUT_DIR / 'Figure5_domain_comparison.png'\n",
    "plt.savefig(output_fig, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Domain comparison plot saved to: {output_fig}\")\n",
    "plt.close()\n",
    "\n",
    "# ==================== SUMMARY REPORT ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìù GENERATING SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_report = f\"\"\"\n",
    "PHASE 1.1: DISTRIBUTIONAL ANALYSIS - SUMMARY REPORT\n",
    "====================================================\n",
    "\n",
    "Dataset Information:\n",
    "-------------------\n",
    "- Total samples: {len(df_clean)}\n",
    "- Panic Disorder cases: {len(df_pd)} ({len(df_pd)/len(df_clean)*100:.2f}%)\n",
    "- Normal cases: {len(df_normal)} ({len(df_normal)/len(df_clean)*100:.2f}%)\n",
    "- Number of features analyzed: {len(features)}\n",
    "\n",
    "Key Findings:\n",
    "-------------\n",
    "\n",
    "1. EFFECT SIZES (Cohen's d):\n",
    "   - Mean |Cohen's d| across all features: {df_results['Cohens_d'].abs().mean():.3f}\n",
    "   - Largest effect size: {df_results.loc[df_results['Cohens_d_abs'].idxmax(), 'Feature']} \n",
    "     ({FEATURE_INFO[df_results.loc[df_results['Cohens_d_abs'].idxmax(), 'Feature']]})\n",
    "     Cohen's d = {df_results['Cohens_d_abs'].max():.3f}\n",
    "   - Number of features with Large effect (|d| > 0.8): {len(df_results[df_results['Cohens_d_abs'] > 0.8])}\n",
    "   - Number of features with Medium effect (0.5 < |d| < 0.8): {len(df_results[(df_results['Cohens_d_abs'] > 0.5) & (df_results['Cohens_d_abs'] <= 0.8)])}\n",
    "   - Number of features with Small effect (0.2 < |d| < 0.5): {len(df_results[(df_results['Cohens_d_abs'] > 0.2) & (df_results['Cohens_d_abs'] <= 0.5)])}\n",
    "\n",
    "2. DISTRIBUTION SEPARATION:\n",
    "   - Mean separation across features: {df_results['Separation_Pct'].mean():.1f}%\n",
    "   - Best separating feature: {df_results.loc[df_results['Separation_Pct'].idxmax(), 'Feature']}\n",
    "     ({FEATURE_INFO[df_results.loc[df_results['Separation_Pct'].idxmax(), 'Feature']]})\n",
    "     Separation = {df_results['Separation_Pct'].max():.1f}%\n",
    "   - Features with >50% separation: {len(df_results[df_results['Separation_Pct'] > 50])}\n",
    "\n",
    "3. STATISTICAL SIGNIFICANCE:\n",
    "   - All features significant (p < 0.05): {len(df_results[df_results['T_Pvalue'] < 0.05]) == len(features)}\n",
    "   - Features with p < 0.001: {len(df_results[df_results['T_Pvalue'] < 0.001])}\n",
    "\n",
    "4. BIOLOGICAL DOMAIN ANALYSIS:\n",
    "   - Biological domain mean effect: {df_domains[df_domains['Domain'] == 'Biological']['Mean_Effect_Size'].values[0]:.3f}\n",
    "   - Psychological domain mean effect: {df_domains[df_domains['Domain'] == 'Psychological']['Mean_Effect_Size'].values[0]:.3f}\n",
    "   - Social domain mean effect: {df_domains[df_domains['Domain'] == 'Social']['Mean_Effect_Size'].values[0]:.3f}\n",
    "\n",
    "Top 5 Features by Discrimination Power:\n",
    "---------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "top5 = df_results.nlargest(5, 'Cohens_d_abs')\n",
    "for i, row in enumerate(top5.itertuples(), 1):\n",
    "    summary_report += f\"\\n{i}. {FEATURE_INFO[row.Feature]}\"\n",
    "    summary_report += f\"\\n   Cohen's d: {row.Cohens_d:.3f} ({row.Effect_Size})\"\n",
    "    summary_report += f\"\\n   Separation: {row.Separation_Pct:.1f}%\"\n",
    "    summary_report += f\"\\n   PD Mean: {row.PD_Mean:.2f} ¬± {row.PD_Std:.2f}\"\n",
    "    summary_report += f\"\\n   Normal Mean: {row.Normal_Mean:.2f} ¬± {row.Normal_Std:.2f}\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "\n",
    "Interpretation:\n",
    "---------------\n",
    "The distributional analysis reveals that while individual features show varying \n",
    "degrees of discrimination between Panic Disorder and Normal cases, NO SINGLE \n",
    "FEATURE achieves perfect separation. This suggests that the 100% accuracy \n",
    "observed in the full model is likely due to SYNERGISTIC INTERACTIONS between \n",
    "multiple features rather than the dominance of any single variable.\n",
    "\n",
    "The biological domain shows {\"stronger\" if df_domains[df_domains['Domain'] == 'Biological']['Mean_Effect_Size'].values[0] > df_domains['Mean_Effect_Size'].mean() else \"comparable\"} \n",
    "discrimination compared to psychological and social domains, suggesting that \n",
    "physiological markers may play an important role in the classification.\n",
    "\n",
    "Next Steps:\n",
    "-----------\n",
    "1. Investigate feature interactions (SHAP analysis)\n",
    "2. Examine decision boundaries using decision trees\n",
    "3. Explore phenotypic clustering within PD cases\n",
    "4. Analyze multivariate patterns using dimensionality reduction\n",
    "\n",
    "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "\n",
    "# Save summary report\n",
    "output_report = OUTPUT_DIR / 'PHASE1_SUMMARY_REPORT.txt'\n",
    "with open(output_report, 'w') as f:\n",
    "    f.write(summary_report)\n",
    "print(f\"\\n‚úÖ Summary report saved to: {output_report}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ PHASE 1.1 COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìÇ All results saved to: {OUTPUT_DIR}\")\n",
    "print(\"\\nüìä Generated outputs:\")\n",
    "print(\"   - statistical_analysis_complete.csv\")\n",
    "print(\"   - Table1_descriptive_statistics.csv\")\n",
    "print(\"   - Table2_domain_analysis.csv\")\n",
    "print(\"   - Figure1_violin_plots_all_features.png\")\n",
    "print(\"   - Figure2_effect_sizes_ranked.png\")\n",
    "print(\"   - Figure3_separation_heatmap.png\")\n",
    "print(\"   - Figure4_distribution_overlaps_top4.png\")\n",
    "print(\"   - Figure5_domain_comparison.png\")\n",
    "print(\"   - PHASE1_SUMMARY_REPORT.txt\")\n",
    "print(\"\\nüöÄ Ready for Phase 1.2: Multidimensional Visualization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1129c044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîç DEEP INVESTIGATION: WHQ_WHD050 (Weight 1 Year Ago)\n",
      "================================================================================\n",
      "\n",
      "üìä Loading datasets...\n",
      "   Full dataset: 6581 samples\n",
      "   Clean dataset: 3279 samples\n",
      "   Dropped: 3302 samples (50.2%)\n",
      "\n",
      "================================================================================\n",
      "üîç MISSING VALUE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Total missing values: 4 (0.06%)\n",
      "\n",
      "Missing in PD group: 0/178 (0.00%)\n",
      "Missing in Normal group: 4/6403 (0.06%)\n",
      "‚ö†Ô∏è  WARNING: Much higher missingness in Normal group!\n",
      "\n",
      "================================================================================\n",
      "üìä BASIC STATISTICS (Clean Data)\n",
      "================================================================================\n",
      "\n",
      " Metric         PD       Normal    Difference\n",
      " Count 115.000000  3164.000000  -3049.000000\n",
      "  Mean 152.695652   574.006321   -421.310669\n",
      "   Std  33.212144  6154.668204  -6121.456060\n",
      "   Min 110.000000    68.000000     42.000000\n",
      "    1% 110.000000    98.000000     12.000000\n",
      "    5% 110.000000   110.000000      0.000000\n",
      "   10% 118.000000   120.000000     -2.000000\n",
      "   25% 120.000000   140.000000    -20.000000\n",
      "   50% 139.000000   165.000000    -26.000000\n",
      "   75% 166.500000   199.000000    -32.500000\n",
      "   90% 200.000000   235.000000    -35.000000\n",
      "   95% 225.000000   250.000000    -25.000000\n",
      "   99% 225.000000   300.000000    -75.000000\n",
      "   Max 225.000000 99999.000000 -99774.000000\n",
      "\n",
      "================================================================================\n",
      "üéØ OUTLIER DETECTION\n",
      "================================================================================\n",
      "\n",
      "PD Group:\n",
      "  IQR bounds: [50.25, 236.25]\n",
      "  Outliers: 0 (0.0%)\n",
      "\n",
      "Normal Group:\n",
      "  IQR bounds: [51.50, 287.50]\n",
      "  Outliers: 62 (2.0%)\n",
      "  Outlier range: [290.00, 99999.00]\n",
      "\n",
      "================================================================================\n",
      "üî¨ VALUE DISTRIBUTION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Unique values analysis:\n",
      "  PD unique values: 20\n",
      "  Normal unique values: 134\n",
      "\n",
      "Most common values (overall):\n",
      "150.0    166\n",
      "160.0    143\n",
      "165.0    129\n",
      "180.0    126\n",
      "140.0    119\n",
      "175.0    115\n",
      "170.0    109\n",
      "200.0    104\n",
      "130.0    100\n",
      "125.0     96\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "üéØ DISCRIMINATIVE VALUE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Bins with highest PD concentration (top 10):\n",
      "           Bin  PD_count  Normal_count    PD_pct  Normal_pct  Ratio_PD_to_Normal\n",
      "[165.0, 170.0]        24           132 20.869565    4.171934            0.181818\n",
      "[120.0, 125.0]        16            99 13.913043    3.128951            0.161616\n",
      "[130.0, 140.0]        24           217 20.869565    6.858407            0.110599\n",
      "[110.0, 120.0]        17           161 14.782609    5.088496            0.105590\n",
      "[220.0, 232.0]         9           173  7.826087    5.467762            0.052023\n",
      "[180.0, 190.0]         7           242  6.086957    7.648546            0.028926\n",
      "[197.5, 206.2]         4           160  3.478261    5.056890            0.025000\n",
      "[190.0, 197.5]         4           166  3.478261    5.246523            0.024096\n",
      "[175.0, 180.0]         3           139  2.608696    4.393173            0.021583\n",
      "[160.0, 165.0]         3           156  2.608696    4.930468            0.019231\n",
      "\n",
      "Bins with highest Normal concentration (bottom 10):\n",
      "             Bin  PD_count  Normal_count   PD_pct  Normal_pct  Ratio_PD_to_Normal\n",
      "  [125.0, 130.0]         1           136 0.869565    4.298357            0.007353\n",
      "  [206.2, 220.0]         1           144 0.869565    4.551201            0.006944\n",
      "  [150.0, 155.0]         1           173 0.869565    5.467762            0.005780\n",
      "  [155.0, 160.0]         1           173 0.869565    5.467762            0.005780\n",
      "  [232.0, 250.0]         0           156 0.000000    4.930468            0.000000\n",
      "   [68.0, 110.0]         0           142 0.000000    4.487990            0.000000\n",
      "  [170.0, 175.0]         0           137 0.000000    4.329962            0.000000\n",
      "  [145.0, 150.0]         0           130 0.000000    4.108723            0.000000\n",
      "  [140.0, 145.0]         0           155 0.000000    4.898862            0.000000\n",
      "[250.0, 99999.0]         0           173 0.000000    5.467762            0.000000\n",
      "\n",
      "================================================================================\n",
      "üé® CREATING VISUALIZATIONS\n",
      "================================================================================\n",
      "\n",
      "üìä Creating detailed distribution plots...\n",
      "‚úÖ Saved: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/investigation_WHD050/Figure1_comprehensive_distribution_analysis.png\n",
      "\n",
      "üìä Creating bin-wise discrimination plot...\n",
      "‚úÖ Saved: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/investigation_WHD050/Figure2_binwise_discrimination.png\n",
      "\n",
      "üìä Creating percentile comparison...\n",
      "‚úÖ Saved: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/investigation_WHD050/Figure3_percentile_comparison.png\n",
      "\n",
      "================================================================================\n",
      "üî¨ ANALYZING SEPARATION MECHANISM\n",
      "================================================================================\n",
      "\n",
      "1. Simple Range Overlap:\n",
      "   PD range: [110.00, 225.00]\n",
      "   Normal range: [68.00, 99999.00]\n",
      "   Overlap range: [110.00, 225.00]\n",
      "   Overlap as % of PD range: 100.0%\n",
      "   Overlap as % of Normal range: 0.1%\n",
      "\n",
      "2. Perfect Cutoff Analysis:\n",
      "   Best single cutoff: 99999.00\n",
      "   Rule: PD>cutoff, Normal<=cutoff\n",
      "   Accuracy: 96.49%\n",
      "   üéØ FOUND NEAR-PERFECT DISCRIMINATOR!\n",
      "\n",
      "================================================================================\n",
      "üìù GENERATING INVESTIGATION REPORT\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Report saved: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/investigation_WHD050/WHD050_INVESTIGATION_REPORT.txt\n",
      "\n",
      "================================================================================\n",
      "‚úÖ INVESTIGATION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üìÇ All results saved to: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/investigation_WHD050\n",
      "\n",
      "üìä Generated files:\n",
      "   - WHD050_INVESTIGATION_REPORT.txt\n",
      "   - Figure1_comprehensive_distribution_analysis.png\n",
      "   - Figure2_binwise_discrimination.png\n",
      "   - Figure3_percentile_comparison.png\n",
      "   - detailed_statistics.csv\n",
      "   - binwise_ratios.csv\n",
      "\n",
      "================================================================================\n",
      "üéØ QUICK SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Best single cutoff accuracy: 96.49%\n",
      "‚ö†Ô∏è  WARNING: This single feature can nearly perfectly discriminate!\n",
      "   This is HIGHLY UNUSUAL and may indicate:\n",
      "   1. A true strong clinical marker\n",
      "   2. Data leakage or collection artifact\n",
      "   3. Need for external validation\n",
      "\n",
      "Missingness: 0.1% (may affect results)\n",
      "Outliers: PD=0, Normal=62\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DEEP INVESTIGATION: WHQ_WHD050 (Weight 1 Year Ago)\n",
    "===================================================\n",
    "This feature shows 98.5% separation but moderate Cohen's d.\n",
    "Something unusual is happening - let's find out what!\n",
    "\n",
    "Author: Panic Disorder ML Investigation\n",
    "Date: 2025-11-11\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "OUTPUT_DIR = Path('/Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/investigation_WHD050')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üîç DEEP INVESTIGATION: WHQ_WHD050 (Weight 1 Year Ago)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ==================== LOAD BOTH DATASETS ====================\n",
    "print(\"\\nüìä Loading datasets...\")\n",
    "\n",
    "# Full dataset (with missing values)\n",
    "df_full = pd.read_csv('/Users/filipecarvalho/Documents/data_science_projects/Panic.3/NHANES_panic_12features.csv')\n",
    "print(f\"   Full dataset: {len(df_full)} samples\")\n",
    "\n",
    "# Clean dataset (used in Phase 1)\n",
    "df_clean = df_full.dropna()\n",
    "print(f\"   Clean dataset: {len(df_clean)} samples\")\n",
    "print(f\"   Dropped: {len(df_full) - len(df_clean)} samples ({(len(df_full) - len(df_clean))/len(df_full)*100:.1f}%)\")\n",
    "\n",
    "feature = 'WHQ_WHD050'\n",
    "\n",
    "# ==================== MISSING VALUE ANALYSIS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç MISSING VALUE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "missing_full = df_full[feature].isnull().sum()\n",
    "missing_pct = (missing_full / len(df_full)) * 100\n",
    "\n",
    "print(f\"\\nTotal missing values: {missing_full} ({missing_pct:.2f}%)\")\n",
    "\n",
    "# Missing by group\n",
    "missing_pd = df_full[df_full['target'] == 1][feature].isnull().sum()\n",
    "missing_normal = df_full[df_full['target'] == 0][feature].isnull().sum()\n",
    "total_pd = len(df_full[df_full['target'] == 1])\n",
    "total_normal = len(df_full[df_full['target'] == 0])\n",
    "\n",
    "print(f\"\\nMissing in PD group: {missing_pd}/{total_pd} ({missing_pd/total_pd*100:.2f}%)\")\n",
    "print(f\"Missing in Normal group: {missing_normal}/{total_normal} ({missing_normal/total_normal*100:.2f}%)\")\n",
    "\n",
    "if missing_pd/total_pd > missing_normal/total_normal * 1.5:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Much higher missingness in PD group!\")\n",
    "elif missing_normal/total_normal > missing_pd/total_pd * 1.5:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Much higher missingness in Normal group!\")\n",
    "\n",
    "# ==================== BASIC STATISTICS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä BASIC STATISTICS (Clean Data)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_pd = df_clean[df_clean['target'] == 1]\n",
    "df_normal = df_clean[df_clean['target'] == 0]\n",
    "\n",
    "pd_vals = df_pd[feature].values\n",
    "normal_vals = df_normal[feature].values\n",
    "\n",
    "stats_dict = {\n",
    "    'Metric': ['Count', 'Mean', 'Std', 'Min', '1%', '5%', '10%', '25%', '50%', '75%', '90%', '95%', '99%', 'Max'],\n",
    "    'PD': [\n",
    "        len(pd_vals),\n",
    "        np.mean(pd_vals),\n",
    "        np.std(pd_vals),\n",
    "        np.min(pd_vals),\n",
    "        np.percentile(pd_vals, 1),\n",
    "        np.percentile(pd_vals, 5),\n",
    "        np.percentile(pd_vals, 10),\n",
    "        np.percentile(pd_vals, 25),\n",
    "        np.percentile(pd_vals, 50),\n",
    "        np.percentile(pd_vals, 75),\n",
    "        np.percentile(pd_vals, 90),\n",
    "        np.percentile(pd_vals, 95),\n",
    "        np.percentile(pd_vals, 99),\n",
    "        np.max(pd_vals)\n",
    "    ],\n",
    "    'Normal': [\n",
    "        len(normal_vals),\n",
    "        np.mean(normal_vals),\n",
    "        np.std(normal_vals),\n",
    "        np.min(normal_vals),\n",
    "        np.percentile(normal_vals, 1),\n",
    "        np.percentile(normal_vals, 5),\n",
    "        np.percentile(normal_vals, 10),\n",
    "        np.percentile(normal_vals, 25),\n",
    "        np.percentile(normal_vals, 50),\n",
    "        np.percentile(normal_vals, 75),\n",
    "        np.percentile(normal_vals, 90),\n",
    "        np.percentile(normal_vals, 95),\n",
    "        np.percentile(normal_vals, 99),\n",
    "        np.max(normal_vals)\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_stats = pd.DataFrame(stats_dict)\n",
    "df_stats['Difference'] = df_stats['PD'] - df_stats['Normal']\n",
    "print(\"\\n\", df_stats.to_string(index=False))\n",
    "\n",
    "# ==================== OUTLIER DETECTION ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ OUTLIER DETECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def detect_outliers_iqr(data, multiplier=1.5):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - multiplier * iqr\n",
    "    upper_bound = q3 + multiplier * iqr\n",
    "    outliers = (data < lower_bound) | (data > upper_bound)\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# PD outliers\n",
    "pd_outliers, pd_lower, pd_upper = detect_outliers_iqr(pd_vals)\n",
    "print(f\"\\nPD Group:\")\n",
    "print(f\"  IQR bounds: [{pd_lower:.2f}, {pd_upper:.2f}]\")\n",
    "print(f\"  Outliers: {pd_outliers.sum()} ({pd_outliers.sum()/len(pd_vals)*100:.1f}%)\")\n",
    "if pd_outliers.sum() > 0:\n",
    "    print(f\"  Outlier values: {pd_vals[pd_outliers]}\")\n",
    "\n",
    "# Normal outliers\n",
    "normal_outliers, normal_lower, normal_upper = detect_outliers_iqr(normal_vals)\n",
    "print(f\"\\nNormal Group:\")\n",
    "print(f\"  IQR bounds: [{normal_lower:.2f}, {normal_upper:.2f}]\")\n",
    "print(f\"  Outliers: {normal_outliers.sum()} ({normal_outliers.sum()/len(normal_vals)*100:.1f}%)\")\n",
    "if normal_outliers.sum() > 5:\n",
    "    print(f\"  Outlier range: [{np.min(normal_vals[normal_outliers]):.2f}, {np.max(normal_vals[normal_outliers]):.2f}]\")\n",
    "\n",
    "# ==================== VALUE DISTRIBUTION ANALYSIS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üî¨ VALUE DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for specific value patterns\n",
    "print(\"\\nUnique values analysis:\")\n",
    "print(f\"  PD unique values: {len(np.unique(pd_vals))}\")\n",
    "print(f\"  Normal unique values: {len(np.unique(normal_vals))}\")\n",
    "\n",
    "# Check for common values\n",
    "all_vals = np.concatenate([pd_vals, normal_vals])\n",
    "value_counts_all = pd.Series(all_vals).value_counts()\n",
    "print(f\"\\nMost common values (overall):\")\n",
    "print(value_counts_all.head(10))\n",
    "\n",
    "# Check for values that are exclusive or heavily skewed to one group\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ DISCRIMINATIVE VALUE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create bins for analysis\n",
    "bins = np.percentile(all_vals, np.linspace(0, 100, 21))  # 20 bins\n",
    "pd_hist, _ = np.histogram(pd_vals, bins=bins)\n",
    "normal_hist, _ = np.histogram(normal_vals, bins=bins)\n",
    "\n",
    "# Find bins with extreme ratios\n",
    "ratios = []\n",
    "for i in range(len(pd_hist)):\n",
    "    if normal_hist[i] > 0 and pd_hist[i] > 0:\n",
    "        ratio = pd_hist[i] / normal_hist[i]\n",
    "        ratios.append({\n",
    "            'Bin': f'[{bins[i]:.1f}, {bins[i+1]:.1f}]',\n",
    "            'PD_count': pd_hist[i],\n",
    "            'Normal_count': normal_hist[i],\n",
    "            'PD_pct': pd_hist[i] / len(pd_vals) * 100,\n",
    "            'Normal_pct': normal_hist[i] / len(normal_vals) * 100,\n",
    "            'Ratio_PD_to_Normal': ratio\n",
    "        })\n",
    "    elif pd_hist[i] > 0:\n",
    "        ratios.append({\n",
    "            'Bin': f'[{bins[i]:.1f}, {bins[i+1]:.1f}]',\n",
    "            'PD_count': pd_hist[i],\n",
    "            'Normal_count': 0,\n",
    "            'PD_pct': pd_hist[i] / len(pd_vals) * 100,\n",
    "            'Normal_pct': 0,\n",
    "            'Ratio_PD_to_Normal': np.inf\n",
    "        })\n",
    "    elif normal_hist[i] > 0:\n",
    "        ratios.append({\n",
    "            'Bin': f'[{bins[i]:.1f}, {bins[i+1]:.1f}]',\n",
    "            'PD_count': 0,\n",
    "            'Normal_count': normal_hist[i],\n",
    "            'PD_pct': 0,\n",
    "            'Normal_pct': normal_hist[i] / len(normal_vals) * 100,\n",
    "            'Ratio_PD_to_Normal': 0\n",
    "        })\n",
    "\n",
    "df_ratios = pd.DataFrame(ratios)\n",
    "df_ratios_sorted = df_ratios.sort_values('Ratio_PD_to_Normal', ascending=False)\n",
    "\n",
    "print(\"\\nBins with highest PD concentration (top 10):\")\n",
    "print(df_ratios_sorted.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nBins with highest Normal concentration (bottom 10):\")\n",
    "print(df_ratios_sorted.tail(10).to_string(index=False))\n",
    "\n",
    "# ==================== VISUALIZATIONS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üé® CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Figure 1: Detailed Distribution Comparison\n",
    "print(\"\\nüìä Creating detailed distribution plots...\")\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Subplot 1: Histograms with KDE\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "ax1.hist(normal_vals, bins=50, alpha=0.5, color='#4ECDC4', label='Normal', density=True, edgecolor='black')\n",
    "ax1.hist(pd_vals, bins=30, alpha=0.5, color='#FF6B6B', label='Panic Disorder', density=True, edgecolor='black')\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "kde_normal = gaussian_kde(normal_vals)\n",
    "kde_pd = gaussian_kde(pd_vals)\n",
    "x_range = np.linspace(min(all_vals), max(all_vals), 1000)\n",
    "ax1.plot(x_range, kde_normal(x_range), color='#4ECDC4', linewidth=2.5, label='Normal KDE')\n",
    "ax1.plot(x_range, kde_pd(x_range), color='#FF6B6B', linewidth=2.5, label='PD KDE')\n",
    "\n",
    "ax1.set_xlabel('Weight 1 Year Ago (lbs)', fontsize=12)\n",
    "ax1.set_ylabel('Density', fontsize=12)\n",
    "ax1.set_title('Distribution Comparison with KDE', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Box plots with outliers\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "bp_data = [normal_vals, pd_vals]\n",
    "bp = ax2.boxplot(bp_data, labels=['Normal', 'PD'], patch_artist=True, showfliers=True)\n",
    "bp['boxes'][0].set_facecolor('#4ECDC4')\n",
    "bp['boxes'][1].set_facecolor('#FF6B6B')\n",
    "ax2.set_ylabel('Weight 1 Year Ago (lbs)', fontsize=12)\n",
    "ax2.set_title('Box Plot Comparison\\n(showing outliers)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Subplot 3: Violin plots\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "parts = ax3.violinplot([normal_vals, pd_vals], positions=[0, 1], showmeans=True, showmedians=True)\n",
    "colors = ['#4ECDC4', '#FF6B6B']\n",
    "for pc, color in zip(parts['bodies'], colors):\n",
    "    pc.set_facecolor(color)\n",
    "    pc.set_alpha(0.7)\n",
    "ax3.set_xticks([0, 1])\n",
    "ax3.set_xticklabels(['Normal', 'PD'])\n",
    "ax3.set_ylabel('Weight 1 Year Ago (lbs)', fontsize=12)\n",
    "ax3.set_title('Violin Plot Comparison', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Subplot 4: Cumulative Distribution Functions\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "normal_sorted = np.sort(normal_vals)\n",
    "pd_sorted = np.sort(pd_vals)\n",
    "normal_cdf = np.arange(1, len(normal_sorted) + 1) / len(normal_sorted)\n",
    "pd_cdf = np.arange(1, len(pd_sorted) + 1) / len(pd_sorted)\n",
    "\n",
    "ax4.plot(normal_sorted, normal_cdf, color='#4ECDC4', linewidth=2, label='Normal')\n",
    "ax4.plot(pd_sorted, pd_cdf, color='#FF6B6B', linewidth=2, label='PD')\n",
    "ax4.set_xlabel('Weight 1 Year Ago (lbs)', fontsize=12)\n",
    "ax4.set_ylabel('Cumulative Probability', fontsize=12)\n",
    "ax4.set_title('Cumulative Distribution Functions', fontsize=14, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Calculate max distance between CDFs (Kolmogorov-Smirnov statistic)\n",
    "ks_stat, ks_pval = stats.ks_2samp(pd_vals, normal_vals)\n",
    "ax4.text(0.05, 0.95, f'KS statistic: {ks_stat:.3f}\\np-value: {ks_pval:.2e}',\n",
    "         transform=ax4.transAxes, fontsize=10, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Subplot 5: Q-Q Plot\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "stats.probplot(pd_vals, dist=\"norm\", plot=ax5)\n",
    "ax5.set_title('Q-Q Plot: PD Group vs Normal Distribution', fontsize=14, fontweight='bold')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 6: Scatter plot with jitter\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "np.random.seed(42)\n",
    "jitter_normal = np.random.normal(0, 0.04, len(normal_vals))\n",
    "jitter_pd = np.random.normal(1, 0.04, len(pd_vals))\n",
    "\n",
    "# Sample if too many points\n",
    "max_points = 500\n",
    "if len(normal_vals) > max_points:\n",
    "    sample_idx = np.random.choice(len(normal_vals), max_points, replace=False)\n",
    "    ax6.scatter(jitter_normal[sample_idx], normal_vals[sample_idx], \n",
    "               alpha=0.3, s=20, color='#4ECDC4', edgecolors='black', linewidths=0.5, label='Normal')\n",
    "else:\n",
    "    ax6.scatter(jitter_normal, normal_vals, \n",
    "               alpha=0.3, s=20, color='#4ECDC4', edgecolors='black', linewidths=0.5, label='Normal')\n",
    "\n",
    "ax6.scatter(jitter_pd, pd_vals, \n",
    "           alpha=0.5, s=30, color='#FF6B6B', edgecolors='black', linewidths=0.5, label='PD')\n",
    "\n",
    "ax6.set_xticks([0, 1])\n",
    "ax6.set_xticklabels(['Normal', 'PD'])\n",
    "ax6.set_ylabel('Weight 1 Year Ago (lbs)', fontsize=12)\n",
    "ax6.set_title('Individual Values (with jitter)', fontsize=14, fontweight='bold')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('WHQ_WHD050: Deep Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "output_fig = OUTPUT_DIR / 'Figure1_comprehensive_distribution_analysis.png'\n",
    "plt.savefig(output_fig, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {output_fig}\")\n",
    "plt.close()\n",
    "\n",
    "# Figure 2: Bin-wise Discrimination\n",
    "print(\"\\nüìä Creating bin-wise discrimination plot...\")\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Top plot: Stacked histogram\n",
    "bin_edges = np.percentile(all_vals, np.linspace(0, 100, 31))\n",
    "ax1.hist([normal_vals, pd_vals], bins=bin_edges, stacked=False, \n",
    "         label=['Normal', 'PD'], color=['#4ECDC4', '#FF6B6B'], \n",
    "         alpha=0.7, edgecolor='black')\n",
    "ax1.set_xlabel('Weight 1 Year Ago (lbs)', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.set_title('Histogram by Group (30 bins)', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Bottom plot: Ratio by bin\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "pd_hist_plot, _ = np.histogram(pd_vals, bins=bin_edges)\n",
    "normal_hist_plot, _ = np.histogram(normal_vals, bins=bin_edges)\n",
    "\n",
    "# Calculate ratio (with smoothing to avoid division by zero)\n",
    "ratio_plot = np.where(normal_hist_plot > 0, \n",
    "                      pd_hist_plot / normal_hist_plot, \n",
    "                      0)\n",
    "\n",
    "colors_ratio = ['red' if r > 1 else 'green' for r in ratio_plot]\n",
    "ax2.bar(bin_centers, ratio_plot, width=np.diff(bin_edges), \n",
    "        alpha=0.7, edgecolor='black', color=colors_ratio)\n",
    "ax2.axhline(y=1, color='black', linestyle='--', linewidth=2, label='Equal distribution')\n",
    "ax2.set_xlabel('Weight 1 Year Ago (lbs)', fontsize=12)\n",
    "ax2.set_ylabel('PD / Normal Ratio', fontsize=12)\n",
    "ax2.set_title('Discrimination Power by Value Range\\n(Red = more PD, Green = more Normal)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "output_fig = OUTPUT_DIR / 'Figure2_binwise_discrimination.png'\n",
    "plt.savefig(output_fig, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {output_fig}\")\n",
    "plt.close()\n",
    "\n",
    "# Figure 3: Percentile Analysis\n",
    "print(\"\\nüìä Creating percentile comparison...\")\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "percentiles = np.arange(0, 101, 5)\n",
    "pd_percentiles = [np.percentile(pd_vals, p) for p in percentiles]\n",
    "normal_percentiles = [np.percentile(normal_vals, p) for p in percentiles]\n",
    "\n",
    "ax.plot(percentiles, pd_percentiles, marker='o', markersize=8, \n",
    "        linewidth=2.5, color='#FF6B6B', label='PD')\n",
    "ax.plot(percentiles, normal_percentiles, marker='s', markersize=8, \n",
    "        linewidth=2.5, color='#4ECDC4', label='Normal')\n",
    "\n",
    "# Fill between to show difference\n",
    "ax.fill_between(percentiles, pd_percentiles, normal_percentiles, \n",
    "                alpha=0.2, color='gray', label='Difference')\n",
    "\n",
    "ax.set_xlabel('Percentile', fontsize=12)\n",
    "ax.set_ylabel('Weight 1 Year Ago (lbs)', fontsize=12)\n",
    "ax.set_title('Percentile Comparison: PD vs Normal', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "output_fig = OUTPUT_DIR / 'Figure3_percentile_comparison.png'\n",
    "plt.savefig(output_fig, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {output_fig}\")\n",
    "plt.close()\n",
    "\n",
    "# ==================== SEPARATION MECHANISM ANALYSIS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üî¨ ANALYZING SEPARATION MECHANISM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate overlap in different ways\n",
    "print(\"\\n1. Simple Range Overlap:\")\n",
    "pd_min, pd_max = np.min(pd_vals), np.max(pd_vals)\n",
    "normal_min, normal_max = np.min(normal_vals), np.max(normal_vals)\n",
    "print(f\"   PD range: [{pd_min:.2f}, {pd_max:.2f}]\")\n",
    "print(f\"   Normal range: [{normal_min:.2f}, {normal_max:.2f}]\")\n",
    "\n",
    "overlap_min = max(pd_min, normal_min)\n",
    "overlap_max = min(pd_max, normal_max)\n",
    "if overlap_max > overlap_min:\n",
    "    overlap_range = overlap_max - overlap_min\n",
    "    pd_range = pd_max - pd_min\n",
    "    normal_range = normal_max - normal_min\n",
    "    print(f\"   Overlap range: [{overlap_min:.2f}, {overlap_max:.2f}]\")\n",
    "    print(f\"   Overlap as % of PD range: {overlap_range/pd_range*100:.1f}%\")\n",
    "    print(f\"   Overlap as % of Normal range: {overlap_range/normal_range*100:.1f}%\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  NO OVERLAP IN RANGES!\")\n",
    "\n",
    "# Check if there's a perfect cutoff\n",
    "print(\"\\n2. Perfect Cutoff Analysis:\")\n",
    "all_vals_sorted = np.sort(np.unique(all_vals))\n",
    "best_cutoff = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for cutoff in all_vals_sorted:\n",
    "    # Try cutoff (PD below, Normal above)\n",
    "    pd_correct = np.sum(pd_vals <= cutoff)\n",
    "    normal_correct = np.sum(normal_vals > cutoff)\n",
    "    accuracy1 = (pd_correct + normal_correct) / (len(pd_vals) + len(normal_vals))\n",
    "    \n",
    "    # Try inverse (PD above, Normal below)\n",
    "    pd_correct_inv = np.sum(pd_vals > cutoff)\n",
    "    normal_correct_inv = np.sum(normal_vals <= cutoff)\n",
    "    accuracy2 = (pd_correct_inv + normal_correct_inv) / (len(pd_vals) + len(normal_vals))\n",
    "    \n",
    "    if accuracy1 > best_accuracy:\n",
    "        best_accuracy = accuracy1\n",
    "        best_cutoff = (cutoff, 'PD<=cutoff, Normal>cutoff')\n",
    "    \n",
    "    if accuracy2 > best_accuracy:\n",
    "        best_accuracy = accuracy2\n",
    "        best_cutoff = (cutoff, 'PD>cutoff, Normal<=cutoff')\n",
    "\n",
    "if best_cutoff:\n",
    "    print(f\"   Best single cutoff: {best_cutoff[0]:.2f}\")\n",
    "    print(f\"   Rule: {best_cutoff[1]}\")\n",
    "    print(f\"   Accuracy: {best_accuracy*100:.2f}%\")\n",
    "    \n",
    "    if best_accuracy > 0.95:\n",
    "        print(\"   üéØ FOUND NEAR-PERFECT DISCRIMINATOR!\")\n",
    "\n",
    "# ==================== SAVE SUMMARY REPORT ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìù GENERATING INVESTIGATION REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "report = f\"\"\"\n",
    "WHQ_WHD050 INVESTIGATION REPORT\n",
    "================================\n",
    "\n",
    "Feature: Weight 1 Year Ago (WHQ_WHD050)\n",
    "Investigation Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "EXECUTIVE SUMMARY\n",
    "-----------------\n",
    "This feature showed 98.5% separation between PD and Normal groups in Phase 1 analysis,\n",
    "despite having only a moderate Cohen's d. This investigation reveals the mechanism.\n",
    "\n",
    "MISSING DATA ANALYSIS\n",
    "---------------------\n",
    "Total missing: {missing_full} ({missing_pct:.2f}%)\n",
    "PD missing: {missing_pd}/{total_pd} ({missing_pd/total_pd*100:.2f}%)\n",
    "Normal missing: {missing_normal}/{total_normal} ({missing_normal/total_normal*100:.2f}%)\n",
    "\n",
    "DESCRIPTIVE STATISTICS\n",
    "----------------------\n",
    "                    PD              Normal          Difference\n",
    "Mean:            {np.mean(pd_vals):8.2f}         {np.mean(normal_vals):8.2f}         {np.mean(pd_vals)-np.mean(normal_vals):8.2f}\n",
    "Median:          {np.median(pd_vals):8.2f}         {np.median(normal_vals):8.2f}         {np.median(pd_vals)-np.median(normal_vals):8.2f}\n",
    "Std:             {np.std(pd_vals):8.2f}         {np.std(normal_vals):8.2f}         {np.std(pd_vals)-np.std(normal_vals):8.2f}\n",
    "Range:           [{pd_min:6.2f}, {pd_max:6.2f}]  [{normal_min:6.2f}, {normal_max:6.2f}]\n",
    "\n",
    "OUTLIER ANALYSIS\n",
    "----------------\n",
    "PD outliers (IQR method): {pd_outliers.sum()} ({pd_outliers.sum()/len(pd_vals)*100:.1f}%)\n",
    "Normal outliers (IQR method): {normal_outliers.sum()} ({normal_outliers.sum()/len(normal_vals)*100:.1f}%)\n",
    "\n",
    "BEST SINGLE CUTOFF\n",
    "------------------\n",
    "Value: {best_cutoff[0]:.2f} lbs\n",
    "Rule: {best_cutoff[1]}\n",
    "Accuracy: {best_accuracy*100:.2f}%\n",
    "\n",
    "STATISTICAL TESTS\n",
    "-----------------\n",
    "Kolmogorov-Smirnov test: D = {ks_stat:.3f}, p < {ks_pval:.2e}\n",
    "{\"HIGHLY SIGNIFICANT DIFFERENCE IN DISTRIBUTIONS\" if ks_pval < 0.001 else \"\"}\n",
    "\n",
    "KEY FINDINGS\n",
    "------------\n",
    "1. {\"CRITICAL: Near-perfect single-variable discrimination found!\" if best_accuracy > 0.95 else \"Moderate single-variable discrimination\"}\n",
    "2. {\"Substantial missingness in data - may affect generalizability\" if missing_pct > 30 else \"Low missingness - data quality good\"}\n",
    "3. {\"Significant outliers detected - may drive separation\" if (pd_outliers.sum() + normal_outliers.sum()) > 50 else \"Outliers minimal\"}\n",
    "4. Distribution shapes: {\"Different\" if ks_pval < 0.001 else \"Similar\"}\n",
    "\n",
    "INTERPRETATION\n",
    "--------------\n",
    "The 98.5% separation observed in Phase 1 analysis appears to be {\"driven by a clear single cutoff threshold\" if best_accuracy > 0.90 else \"due to distributional differences rather than a single threshold\"}.\n",
    "\n",
    "This {'suggests a strong univariate discriminator' if best_accuracy > 0.90 else 'indicates complex multivariate interactions'}.\n",
    "\n",
    "RECOMMENDATION\n",
    "--------------\n",
    "{'‚ö†Ô∏è  This feature alone provides near-perfect classification. Investigate if it should be excluded or if it represents true clinical signal.' if best_accuracy > 0.95 else '‚úì This feature contributes meaningfully but not dominantly to the model.'}\n",
    "\n",
    "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "\n",
    "output_report = OUTPUT_DIR / 'WHD050_INVESTIGATION_REPORT.txt'\n",
    "with open(output_report, 'w') as f:\n",
    "    f.write(report)\n",
    "print(f\"\\n‚úÖ Report saved: {output_report}\")\n",
    "\n",
    "# Save detailed statistics\n",
    "df_stats.to_csv(OUTPUT_DIR / 'detailed_statistics.csv', index=False)\n",
    "df_ratios.to_csv(OUTPUT_DIR / 'binwise_ratios.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ INVESTIGATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìÇ All results saved to: {OUTPUT_DIR}\")\n",
    "print(\"\\nüìä Generated files:\")\n",
    "print(\"   - WHD050_INVESTIGATION_REPORT.txt\")\n",
    "print(\"   - Figure1_comprehensive_distribution_analysis.png\")\n",
    "print(\"   - Figure2_binwise_discrimination.png\")\n",
    "print(\"   - Figure3_percentile_comparison.png\")\n",
    "print(\"   - detailed_statistics.csv\")\n",
    "print(\"   - binwise_ratios.csv\")\n",
    "\n",
    "# Print summary to console\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ QUICK SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBest single cutoff accuracy: {best_accuracy*100:.2f}%\")\n",
    "if best_accuracy > 0.95:\n",
    "    print(\"‚ö†Ô∏è  WARNING: This single feature can nearly perfectly discriminate!\")\n",
    "    print(\"   This is HIGHLY UNUSUAL and may indicate:\")\n",
    "    print(\"   1. A true strong clinical marker\")\n",
    "    print(\"   2. Data leakage or collection artifact\")\n",
    "    print(\"   3. Need for external validation\")\n",
    "print(f\"\\nMissingness: {missing_pct:.1f}% (may affect results)\")\n",
    "print(f\"Outliers: PD={pd_outliers.sum()}, Normal={normal_outliers.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5820a6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üßπ DATASET CLEANUP: Removing WHQ_WHD050\n",
      "================================================================================\n",
      "\n",
      "üìä Loading original dataset...\n",
      "   ‚úÖ Loaded: 6581 samples √ó 13 columns\n",
      "\n",
      "================================================================================\n",
      "üìã DEFINING 11 CLEAN FEATURES\n",
      "================================================================================\n",
      "\n",
      "‚úÖ 11 Clean Features Selected:\n",
      "    1. BIX_BIDFAT\n",
      "    2. DEMO_INDFMPIR\n",
      "    3. DEMO_RIDAGEMN\n",
      "    4. DEMO_RIAGENDR\n",
      "    5. BPX_BPXDAR\n",
      "    6. DEMO_DMDHHSIZ\n",
      "    7. BMX_BMXBMI\n",
      "    8. ALQ_ALQ130\n",
      "    9. MCQ_MCQ250F\n",
      "   10. MPQ_MPQ070\n",
      "   11. DEMO_DMDMARTL\n",
      "\n",
      "‚ùå Removed Feature:\n",
      "    WHQ_WHD050 (Weight 1 Year Ago) - Contains 99999 coded values\n",
      "\n",
      "================================================================================\n",
      "üì¶ CREATING CLEAN DATASET\n",
      "================================================================================\n",
      "\n",
      "‚úÖ New dataset shape: 6581 samples √ó 12 columns\n",
      "   Features: 11\n",
      "   Target: 1\n",
      "\n",
      "================================================================================\n",
      "üîç DATA QUALITY CHECK\n",
      "================================================================================\n",
      "\n",
      "1. Missing Values:\n",
      "   Total missing values: 5079\n",
      "\n",
      "   Missing values by feature:\n",
      "      BIX_BIDFAT          : 1901 (28.89%)\n",
      "      DEMO_INDFMPIR       :  423 ( 6.43%)\n",
      "      BPX_BPXDAR          :  429 ( 6.52%)\n",
      "      BMX_BMXBMI          :  233 ( 3.54%)\n",
      "      ALQ_ALQ130          : 1917 (29.13%)\n",
      "      DEMO_DMDMARTL       :  176 ( 2.67%)\n",
      "\n",
      "2. Checking for Suspicious Coded Values (99999, 9999, 999):\n",
      "   ‚ö†Ô∏è  BPX_BPXDAR:\n",
      "      Value 99: 2 occurrences (0.03%)\n",
      "      Value 77: 148 occurrences (2.41%)\n",
      "      Value 66: 236 occurrences (3.84%)\n",
      "   ‚ö†Ô∏è  ALQ_ALQ130:\n",
      "      Value 99: 4 occurrences (0.09%)\n",
      "\n",
      "3. Target Distribution:\n",
      "   Normal          (target=0):  6403 (97.30%)\n",
      "   Panic Disorder  (target=1):   178 ( 2.70%)\n",
      "\n",
      "4. Basic Statistics for 11 Features:\n",
      "       BIX_BIDFAT  DEMO_INDFMPIR  DEMO_RIDAGEMN  DEMO_RIAGENDR  BPX_BPXDAR  \\\n",
      "count     4680.00        6158.00        6581.00        6581.00     6152.00   \n",
      "mean        23.93           2.80         354.52           1.55       68.78   \n",
      "std         11.85           1.67          69.71           0.50       12.01   \n",
      "min          1.80           0.00         240.00           1.00        0.00   \n",
      "25%         15.80           1.31         294.00           1.00       62.00   \n",
      "50%         21.80           2.61         354.00           2.00       69.00   \n",
      "75%         29.20           4.64         418.00           2.00       76.00   \n",
      "max         73.70           5.00         479.00           2.00      121.00   \n",
      "\n",
      "       DEMO_DMDHHSIZ  BMX_BMXBMI  ALQ_ALQ130  MCQ_MCQ250F  MPQ_MPQ070  \\\n",
      "count        6581.00     6348.00     4664.00      6581.00     6581.00   \n",
      "mean            3.40       27.80        3.30         1.60        1.60   \n",
      "std             1.59        7.10        3.93         0.49        0.49   \n",
      "min             1.00       14.42        1.00         1.00        1.00   \n",
      "25%             2.00       22.81        2.00         1.00        1.00   \n",
      "50%             3.00       26.11        2.00         2.00        2.00   \n",
      "75%             4.00       30.98        4.00         2.00        2.00   \n",
      "max             7.00       64.97       99.00         2.00        2.00   \n",
      "\n",
      "       DEMO_DMDMARTL  \n",
      "count        6405.00  \n",
      "mean            3.16  \n",
      "std             2.02  \n",
      "min             1.00  \n",
      "25%             1.00  \n",
      "50%             3.00  \n",
      "75%             5.00  \n",
      "max             6.00  \n",
      "\n",
      "================================================================================\n",
      "üíæ SAVING CLEAN DATASET\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Clean dataset saved to:\n",
      "   /Users/filipecarvalho/Documents/data_science_projects/Panic.3/NHANES_panic_11features_CLEAN.csv\n",
      "\n",
      "‚úÖ Summary saved to:\n",
      "   /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/DATASET_CLEANUP_SUMMARY.txt\n",
      "\n",
      "================================================================================\n",
      "üìä COMPARISON WITH ORIGINAL DATASET\n",
      "================================================================================\n",
      "\n",
      "Original (12 features): 6581 √ó 13\n",
      "Clean (11 features):    6581 √ó 12\n",
      "Reduction: 1 column removed\n",
      "\n",
      "================================================================================\n",
      "‚úÖ CLEANUP COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üìã NEXT STEPS:\n",
      "\n",
      "1. ü§ñ RE-TRAIN MODEL:\n",
      "   - Load NHANES_panic_11features_CLEAN.csv\n",
      "   - Train same model architecture (XGBoost, Random Forest, etc.)\n",
      "   - Compare accuracy with original 12-feature model\n",
      "   - Save new model as 'panic_model_11features_CLEAN.joblib'\n",
      "\n",
      "2. üìä RE-RUN PHASE 1.1 ANALYSIS:\n",
      "   - Use the 11-feature dataset\n",
      "   - Generate new distribution plots\n",
      "   - Compare effect sizes without WHQ_WHD050\n",
      "\n",
      "3. üîç VERIFY MODEL BEHAVIOR:\n",
      "   - Check if 100% accuracy is maintained\n",
      "   - Analyze feature importances in new model\n",
      "   - Verify no other features have suspicious patterns\n",
      "\n",
      "4. üìù UPDATE PAPER:\n",
      "   - Document data cleaning process\n",
      "   - Explain removal of WHQ_WHD050\n",
      "   - Present results from clean 11-feature model\n",
      "   - Discuss implications for findings\n",
      "\n",
      "üéØ KEY QUESTION:\n",
      "   Does the model still achieve near-perfect separation with 11 clean features?\n",
      "   This will determine if the phenomenon is real or artifact-driven!\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DATASET CLEANUP: Remove Contaminated Feature WHQ_WHD050\n",
    "========================================================\n",
    "WHQ_WHD050 (Weight 1 Year Ago) contains 99999 coded values that create\n",
    "artificial separation (96.49% accuracy). We remove this feature to work\n",
    "with clean data only.\n",
    "\n",
    "Author: Panic Disorder ML Investigation\n",
    "Date: 2025-11-11\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üßπ DATASET CLEANUP: Removing WHQ_WHD050\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ==================== LOAD ORIGINAL DATA ====================\n",
    "print(\"\\nüìä Loading original dataset...\")\n",
    "DATA_PATH = '/Users/filipecarvalho/Documents/data_science_projects/Panic.3/NHANES_panic_12features.csv'\n",
    "df_original = pd.read_csv(DATA_PATH)\n",
    "print(f\"   ‚úÖ Loaded: {df_original.shape[0]} samples √ó {df_original.shape[1]} columns\")\n",
    "\n",
    "# ==================== DEFINE 11 CLEAN FEATURES ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã DEFINING 11 CLEAN FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Original 12 features (ordered by importance from Paper 2)\n",
    "features_original = [\n",
    "    'BIX_BIDFAT',      # 1. Body Fat Mass (23.8%)\n",
    "    'DEMO_INDFMPIR',   # 2. Poverty Income Ratio (17.4%)\n",
    "    'DEMO_RIDAGEMN',   # 3. Age in months (10.9%)\n",
    "    'DEMO_RIAGENDR',   # 4. Gender (10.6%)\n",
    "    'BPX_BPXDAR',      # 5. Diastolic BP (9.8%)\n",
    "    'WHQ_WHD050',      # 6. Weight 1 Year Ago (8.0%) ‚ö†Ô∏è CONTAMINATED - REMOVING\n",
    "    'DEMO_DMDHHSIZ',   # 7. Household Size (7.9%)\n",
    "    'BMX_BMXBMI',      # 8. BMI (7.4%)\n",
    "    'ALQ_ALQ130',      # 9. Alcohol Consumption (2.6%)\n",
    "    'MCQ_MCQ250F',     # 10. Family History HTN/Stroke (0.9%)\n",
    "    'MPQ_MPQ070',      # 11. Lower Back Pain (0.5%)\n",
    "    'DEMO_DMDMARTL'    # 12. Marital Status (0.3%)\n",
    "]\n",
    "\n",
    "# New 11 features (removing WHQ_WHD050)\n",
    "features_clean = [\n",
    "    'BIX_BIDFAT',      # 1. Body Fat Mass\n",
    "    'DEMO_INDFMPIR',   # 2. Poverty Income Ratio\n",
    "    'DEMO_RIDAGEMN',   # 3. Age in months\n",
    "    'DEMO_RIAGENDR',   # 4. Gender\n",
    "    'BPX_BPXDAR',      # 5. Diastolic BP\n",
    "    'DEMO_DMDHHSIZ',   # 6. Household Size\n",
    "    'BMX_BMXBMI',      # 7. BMI\n",
    "    'ALQ_ALQ130',      # 8. Alcohol Consumption\n",
    "    'MCQ_MCQ250F',     # 9. Family History HTN/Stroke\n",
    "    'MPQ_MPQ070',      # 10. Lower Back Pain\n",
    "    'DEMO_DMDMARTL'    # 11. Marital Status\n",
    "]\n",
    "\n",
    "print(\"\\n‚úÖ 11 Clean Features Selected:\")\n",
    "for i, feat in enumerate(features_clean, 1):\n",
    "    print(f\"   {i:2d}. {feat}\")\n",
    "\n",
    "print(\"\\n‚ùå Removed Feature:\")\n",
    "print(\"    WHQ_WHD050 (Weight 1 Year Ago) - Contains 99999 coded values\")\n",
    "\n",
    "# ==================== CREATE CLEAN DATASET ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì¶ CREATING CLEAN DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select only the 11 clean features + target\n",
    "df_clean = df_original[features_clean + ['target']].copy()\n",
    "\n",
    "print(f\"\\n‚úÖ New dataset shape: {df_clean.shape[0]} samples √ó {df_clean.shape[1]} columns\")\n",
    "print(f\"   Features: {len(features_clean)}\")\n",
    "print(f\"   Target: 1\")\n",
    "\n",
    "# ==================== DATA QUALITY CHECK ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç DATA QUALITY CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n1. Missing Values:\")\n",
    "missing_counts = df_clean.isnull().sum()\n",
    "total_missing = missing_counts.sum()\n",
    "print(f\"   Total missing values: {total_missing}\")\n",
    "\n",
    "if total_missing > 0:\n",
    "    print(\"\\n   Missing values by feature:\")\n",
    "    for feat in features_clean:\n",
    "        missing = missing_counts[feat]\n",
    "        if missing > 0:\n",
    "            pct = (missing / len(df_clean)) * 100\n",
    "            print(f\"      {feat:20s}: {missing:4d} ({pct:5.2f}%)\")\n",
    "else:\n",
    "    print(\"   ‚úÖ No missing values in any feature!\")\n",
    "\n",
    "# Check for suspicious values (99999, 9999, etc)\n",
    "print(\"\\n2. Checking for Suspicious Coded Values (99999, 9999, 999):\")\n",
    "suspicious_found = False\n",
    "\n",
    "for feat in features_clean:\n",
    "    # Check for extreme values that might be codes\n",
    "    feat_values = df_clean[feat].values\n",
    "    feat_values_no_nan = feat_values[~np.isnan(feat_values)]\n",
    "    \n",
    "    if len(feat_values_no_nan) > 0:\n",
    "        max_val = np.max(feat_values_no_nan)\n",
    "        \n",
    "        # Check for common NHANES missing codes\n",
    "        suspicious_codes = [99999, 9999, 999, 99, 77, 66]\n",
    "        found_codes = []\n",
    "        \n",
    "        for code in suspicious_codes:\n",
    "            count = np.sum(feat_values_no_nan == code)\n",
    "            if count > 0:\n",
    "                found_codes.append((code, count))\n",
    "        \n",
    "        if found_codes:\n",
    "            suspicious_found = True\n",
    "            print(f\"   ‚ö†Ô∏è  {feat}:\")\n",
    "            for code, count in found_codes:\n",
    "                print(f\"      Value {code}: {count} occurrences ({count/len(feat_values_no_nan)*100:.2f}%)\")\n",
    "\n",
    "if not suspicious_found:\n",
    "    print(\"   ‚úÖ No suspicious coded values detected!\")\n",
    "\n",
    "# Check target distribution\n",
    "print(\"\\n3. Target Distribution:\")\n",
    "target_counts = df_clean['target'].value_counts().sort_index()\n",
    "for val, count in target_counts.items():\n",
    "    label = \"Normal\" if val == 0 else \"Panic Disorder\"\n",
    "    pct = (count / len(df_clean)) * 100\n",
    "    print(f\"   {label:15s} (target={val}): {count:5d} ({pct:5.2f}%)\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n4. Basic Statistics for 11 Features:\")\n",
    "print(df_clean[features_clean].describe().round(2))\n",
    "\n",
    "# ==================== SAVE CLEAN DATASET ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ SAVING CLEAN DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save with descriptive name\n",
    "output_path = '/Users/filipecarvalho/Documents/data_science_projects/Panic.3/NHANES_panic_11features_CLEAN.csv'\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "print(f\"\\n‚úÖ Clean dataset saved to:\")\n",
    "print(f\"   {output_path}\")\n",
    "\n",
    "# Also save a summary file\n",
    "summary_path = Path('/Users/filipecarvalho/Documents/data_science_projects/Panic.3/results')\n",
    "summary_path.mkdir(exist_ok=True)\n",
    "\n",
    "summary_file = summary_path / 'DATASET_CLEANUP_SUMMARY.txt'\n",
    "summary_text = f\"\"\"\n",
    "DATASET CLEANUP SUMMARY\n",
    "========================\n",
    "\n",
    "Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "CHANGES MADE\n",
    "------------\n",
    "Original dataset: NHANES_panic_12features.csv\n",
    "New dataset: NHANES_panic_11features_CLEAN.csv\n",
    "\n",
    "REMOVED FEATURE\n",
    "---------------\n",
    "WHQ_WHD050 (Weight 1 Year Ago)\n",
    "\n",
    "Reason: Contains 99999 coded values (NHANES \"Don't Know/Refused\" code)\n",
    "        that created artificial 96.49% discrimination between groups.\n",
    "        This is a data quality artifact, not a true clinical signal.\n",
    "\n",
    "11 CLEAN FEATURES RETAINED\n",
    "---------------------------\n",
    "1. BIX_BIDFAT      - Body Fat Mass (kg)\n",
    "2. DEMO_INDFMPIR   - Poverty Income Ratio\n",
    "3. DEMO_RIDAGEMN   - Age (months)\n",
    "4. DEMO_RIAGENDR   - Gender\n",
    "5. BPX_BPXDAR      - Diastolic Blood Pressure\n",
    "6. DEMO_DMDHHSIZ   - Household Size\n",
    "7. BMX_BMXBMI      - Body Mass Index\n",
    "8. ALQ_ALQ130      - Alcohol Consumption (drinks/day)\n",
    "9. MCQ_MCQ250F     - Family History HTN/Stroke\n",
    "10. MPQ_MPQ070     - Lower Back Pain (last 3 months)\n",
    "11. DEMO_DMDMARTL  - Marital Status\n",
    "\n",
    "DATASET STATISTICS\n",
    "------------------\n",
    "Total samples: {len(df_clean)}\n",
    "Features: 11\n",
    "Target variable: 1 (CIDPSCOR)\n",
    "\n",
    "Target Distribution:\n",
    "- Panic Disorder: {target_counts[1]} ({target_counts[1]/len(df_clean)*100:.2f}%)\n",
    "- Normal: {target_counts[0]} ({target_counts[0]/len(df_clean)*100:.2f}%)\n",
    "\n",
    "Missing Values: {total_missing} ({total_missing/(len(df_clean)*len(features_clean))*100:.3f}% of all values)\n",
    "\n",
    "NEXT STEPS\n",
    "----------\n",
    "1. Re-train the machine learning model with 11 clean features\n",
    "2. Verify if 100% accuracy is maintained with clean data\n",
    "3. Re-run Phase 1.1 distributional analysis with 11 features\n",
    "4. Compare new results with original 12-feature results\n",
    "5. Update paper methodology to document this data cleaning step\n",
    "\n",
    "IMPACT ASSESSMENT\n",
    "-----------------\n",
    "The removal of WHQ_WHD050 is expected to:\n",
    "- Reduce artificial discrimination power\n",
    "- Provide more realistic model performance estimates\n",
    "- Improve model generalizability\n",
    "- Ensure clinical validity of findings\n",
    "\n",
    "If the model still achieves near-perfect accuracy with 11 clean features,\n",
    "this strengthens the validity of the findings. If accuracy drops significantly,\n",
    "this indicates the original results were partly driven by data artifacts.\n",
    "\n",
    "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(summary_text)\n",
    "print(f\"\\n‚úÖ Summary saved to:\")\n",
    "print(f\"   {summary_file}\")\n",
    "\n",
    "# ==================== COMPARISON WITH ORIGINAL ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä COMPARISON WITH ORIGINAL DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nOriginal (12 features): {df_original.shape[0]} √ó {df_original.shape[1]}\")\n",
    "print(f\"Clean (11 features):    {df_clean.shape[0]} √ó {df_clean.shape[1]}\")\n",
    "print(f\"Reduction: {df_original.shape[1] - df_clean.shape[1]} column removed\")\n",
    "\n",
    "# ==================== FINAL INSTRUCTIONS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ CLEANUP COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìã NEXT STEPS:\")\n",
    "print(\"\\n1. ü§ñ RE-TRAIN MODEL:\")\n",
    "print(\"   - Load NHANES_panic_11features_CLEAN.csv\")\n",
    "print(\"   - Train same model architecture (XGBoost, Random Forest, etc.)\")\n",
    "print(\"   - Compare accuracy with original 12-feature model\")\n",
    "print(\"   - Save new model as 'panic_model_11features_CLEAN.joblib'\")\n",
    "\n",
    "print(\"\\n2. üìä RE-RUN PHASE 1.1 ANALYSIS:\")\n",
    "print(\"   - Use the 11-feature dataset\")\n",
    "print(\"   - Generate new distribution plots\")\n",
    "print(\"   - Compare effect sizes without WHQ_WHD050\")\n",
    "\n",
    "print(\"\\n3. üîç VERIFY MODEL BEHAVIOR:\")\n",
    "print(\"   - Check if 100% accuracy is maintained\")\n",
    "print(\"   - Analyze feature importances in new model\")\n",
    "print(\"   - Verify no other features have suspicious patterns\")\n",
    "\n",
    "print(\"\\n4. üìù UPDATE PAPER:\")\n",
    "print(\"   - Document data cleaning process\")\n",
    "print(\"   - Explain removal of WHQ_WHD050\")\n",
    "print(\"   - Present results from clean 11-feature model\")\n",
    "print(\"   - Discuss implications for findings\")\n",
    "\n",
    "print(\"\\nüéØ KEY QUESTION:\")\n",
    "print(\"   Does the model still achieve near-perfect separation with 11 clean features?\")\n",
    "print(\"   This will determine if the phenomenon is real or artifact-driven!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b7df26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ü§ñ RE-TRAINING MODEL WITH 11 CLEAN FEATURES\n",
      "================================================================================\n",
      "\n",
      "üìÇ Data: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/NHANES_panic_11features_CLEAN.csv\n",
      "üìÇ Output: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/model_retrain_11features\n",
      "\n",
      "üìã Using 11 clean features\n",
      "üé≤ Random state: 42\n",
      "üìä Test size: 25.0%\n",
      "\n",
      "================================================================================\n",
      "1. LOADING CLEAN DATASET\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Loaded: 6581 samples √ó 12 columns\n",
      "‚úÖ All 11 features present\n",
      "\n",
      "üéØ Target distribution:\n",
      "   Normal          (target=0):  6403 (97.30%)\n",
      "   Panic Disorder  (target=1):   178 ( 2.70%)\n",
      "\n",
      "üìä PD Prevalence: 2.70%\n",
      "\n",
      "================================================================================\n",
      "2. PREPROCESSING\n",
      "================================================================================\n",
      "\n",
      "üîß Imputing missing values with median (same as original)...\n",
      "   Imputed BPX_BPXDAR\n",
      "   Imputed DEMO_INDFMPIR\n",
      "   Imputed ALQ_ALQ130\n",
      "   Imputed BMX_BMXBMI\n",
      "   Imputed DEMO_DMDMARTL\n",
      "   Imputed BIX_BIDFAT\n",
      "\n",
      "‚úÖ No missing values after imputation\n",
      "\n",
      "‚úÖ Final dataset:\n",
      "   X shape: (6581, 11)\n",
      "   y shape: (6581,)\n",
      "   Features: 11\n",
      "\n",
      "================================================================================\n",
      "3. TRAIN/TEST SPLIT\n",
      "================================================================================\n",
      "\n",
      "üìä Split summary:\n",
      "   Train: 4935 samples (133 PD, 4802 Normal)\n",
      "   Test:  1646 samples (45 PD, 1601 Normal)\n",
      "\n",
      "   PD prevalence in train: 2.70%\n",
      "   PD prevalence in test:  2.73%\n",
      "\n",
      "================================================================================\n",
      "4. BUILDING MODEL PIPELINE\n",
      "================================================================================\n",
      "\n",
      "üèóÔ∏è  Pipeline components:\n",
      "   1. StandardScaler (normalize features)\n",
      "   2. GradientBoostingClassifier\n",
      "\n",
      "‚öôÔ∏è  Model hyperparameters:\n",
      "   n_estimators        : 200\n",
      "   max_depth           : 8\n",
      "   learning_rate       : 0.1\n",
      "   min_samples_leaf    : 1\n",
      "   min_samples_split   : 4\n",
      "   random_state        : 42\n",
      "\n",
      "‚úÖ Pipeline created successfully\n",
      "\n",
      "================================================================================\n",
      "5. TRAINING MODEL\n",
      "================================================================================\n",
      "\n",
      "‚öñÔ∏è  Computing sample weights (balanced)...\n",
      "   Sample weights computed for 4935 training samples\n",
      "\n",
      "üéØ Training GradientBoostingClassifier...\n",
      "   (This may take 30-60 seconds...)\n",
      "\n",
      "‚úÖ Model trained successfully!\n",
      "\n",
      "================================================================================\n",
      "6. GENERATING PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Predictions generated for test set\n",
      "   Binary predictions: 1646\n",
      "   Probability scores: 1646\n",
      "\n",
      "================================================================================\n",
      "7. PERFORMANCE EVALUATION\n",
      "================================================================================\n",
      "\n",
      "üìä CLASSIFICATION METRICS:\n",
      "   Accuracy:  0.9939 (99.39%)\n",
      "   Precision: 1.0000 (100.00%)\n",
      "   Recall:    0.7778 (77.78%)\n",
      "   F1-Score:  0.8750\n",
      "   ROC-AUC:   0.9343\n",
      "\n",
      "üìä CONFUSION MATRIX:\n",
      "   True Negatives:  1601\n",
      "   False Positives: 0\n",
      "   False Negatives: 10\n",
      "   True Positives:  35\n",
      "\n",
      "üìä DETAILED CLASSIFICATION REPORT:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        Normal     0.9938    1.0000    0.9969      1601\n",
      "Panic Disorder     1.0000    0.7778    0.8750        45\n",
      "\n",
      "      accuracy                         0.9939      1646\n",
      "     macro avg     0.9969    0.8889    0.9359      1646\n",
      "  weighted avg     0.9940    0.9939    0.9936      1646\n",
      "\n",
      "\n",
      "üìä PROBABILITY DISTRIBUTION:\n",
      "   Min probability:    0.0000\n",
      "   Max probability:    1.0000\n",
      "   Mean probability:   0.0216\n",
      "   Median probability: 0.0000\n",
      "\n",
      "üìä PROBABILITY RANGE DISTRIBUTION:\n",
      "   < 1%      : 1609 cases (97.75%)\n",
      "   1-10%     :    0 cases ( 0.00%)\n",
      "   10-50%    :    2 cases ( 0.12%)\n",
      "   50-90%    :    0 cases ( 0.00%)\n",
      "   90-99%    :    0 cases ( 0.00%)\n",
      "   > 99%     :   35 cases ( 2.13%)\n",
      "\n",
      "üéØ DECISION CONFIDENCE:\n",
      "   Extreme predictions (< 10% or > 90%): 1644 (99.88%)\n",
      "   Borderline (10-90%): 2 (0.12%)\n",
      "\n",
      "   ‚úÖ Very few borderline cases - Strong separation!\n",
      "\n",
      "================================================================================\n",
      "8. CRITICAL ASSESSMENT\n",
      "================================================================================\n",
      "\n",
      "üîç COMPARISON WITH ORIGINAL MODEL (12 features with WHQ_WHD050):\n",
      "\n",
      "   Original model metrics:\n",
      "   - Expected accuracy: ~100% (reported in Phase 1)\n",
      "   - Expected borderline: 0 cases\n",
      "   - Expected ROC-AUC: 1.000\n",
      "\n",
      "   New model metrics (11 clean features):\n",
      "   - Accuracy:   0.9939 (99.39%)\n",
      "   - Borderline: 2 cases\n",
      "   - ROC-AUC:    0.9343\n",
      "\n",
      "   ‚ö†Ô∏è  RESULT: Near-perfect accuracy maintained\n",
      "   ‚Üí Slight decrease from original (~0.6% error rate)\n",
      "   ‚Üí WHQ_WHD050 contributed but was not critical\n",
      "   ‚Üí Findings remain strong with clean data\n",
      "\n",
      "================================================================================\n",
      "9. FEATURE IMPORTANCE\n",
      "================================================================================\n",
      "\n",
      "üìä Top 10 Most Important Features:\n",
      "    9. BIX_BIDFAT          : 0.2467 (24.67%)\n",
      "    4. DEMO_INDFMPIR       : 0.1838 (18.38%)\n",
      "    1. BPX_BPXDAR          : 0.1152 (11.52%)\n",
      "    8. DEMO_RIAGENDR       : 0.1082 (10.82%)\n",
      "    6. BMX_BMXBMI          : 0.0980 (9.80%)\n",
      "    3. DEMO_RIDAGEMN       : 0.0865 (8.65%)\n",
      "   11. DEMO_DMDHHSIZ       : 0.0803 (8.03%)\n",
      "    5. ALQ_ALQ130          : 0.0542 (5.42%)\n",
      "    7. DEMO_DMDMARTL       : 0.0135 (1.35%)\n",
      "   10. MCQ_MCQ250F         : 0.0090 (0.90%)\n",
      "\n",
      "   ‚úÖ No single feature dominance (max = 24.7%)\n",
      "\n",
      "================================================================================\n",
      "10. GENERATING VISUALIZATIONS\n",
      "================================================================================\n",
      "\n",
      "üìä Creating ROC curve...\n",
      "   ‚úÖ Saved: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/model_retrain_11features/Figure1_ROC_curve_11features.png\n",
      "\n",
      "üìä Creating probability distribution plot...\n",
      "   ‚úÖ Saved: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/model_retrain_11features/Figure2_probability_distribution_confusion.png\n",
      "\n",
      "üìä Creating feature importance plot...\n",
      "   ‚úÖ Saved: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/model_retrain_11features/Figure3_feature_importance_11features.png\n",
      "\n",
      "üìä Creating prediction scatter plot...\n",
      "   ‚úÖ Saved: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/model_retrain_11features/Figure4_prediction_scatter.png\n",
      "\n",
      "================================================================================\n",
      "11. SAVING RESULTS\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Model saved: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/model_retrain_11features/panic_model_11features_CLEAN.joblib\n",
      "‚úÖ Predictions saved: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/model_retrain_11features/predictions_11features.csv\n",
      "‚úÖ Feature importance saved: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/model_retrain_11features/feature_importance_11features.csv\n",
      "‚úÖ Metrics saved: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/model_retrain_11features/metrics_summary_11features.csv\n",
      "\n",
      "================================================================================\n",
      "12. GENERATING FINAL REPORT\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Report saved: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/model_retrain_11features/MODEL_RETRAIN_REPORT.txt\n",
      "\n",
      "================================================================================\n",
      "‚úÖ MODEL RE-TRAINING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üéØ FINAL RESULT: STRONG\n",
      "\n",
      "üìä Key Metrics:\n",
      "   Accuracy:  99.39%\n",
      "   ROC-AUC:   0.9343\n",
      "   Borderline: 2 cases\n",
      "\n",
      "‚úÖ Near-perfect accuracy maintained!\n",
      "   ‚úì Findings remain strong\n",
      "   ‚úì Minor adjustments to paper needed\n",
      "\n",
      "üìÇ All results saved to: /Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/model_retrain_11features\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RE-TRAINING MODEL WITH 11 CLEAN FEATURES\n",
    "=========================================\n",
    "Testing if 100% accuracy is maintained after removing WHQ_WHD050\n",
    "\n",
    "Uses EXACT same pipeline, hyperparameters, and preprocessing as original model\n",
    "to ensure fair comparison.\n",
    "\n",
    "Original: 12 features (including contaminated WHQ_WHD050)\n",
    "New: 11 clean features (WHQ_WHD050 removed)\n",
    "\n",
    "Author: Panic Disorder ML Investigation\n",
    "Date: 2025-11-11\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_auc_score, roc_curve, confusion_matrix,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ü§ñ RE-TRAINING MODEL WITH 11 CLEAN FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = '/Users/filipecarvalho/Documents/data_science_projects/Panic.3/NHANES_panic_11features_CLEAN.csv'\n",
    "OUTPUT_DIR = Path('/Users/filipecarvalho/Documents/data_science_projects/Panic.3/results/model_retrain_11features')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 11 Clean Features (EXACT same order as original, minus WHQ_WHD050)\n",
    "FEATURES_CLEAN = [\n",
    "    'BPX_BPXDAR',      # Diastolic BP\n",
    "    'MPQ_MPQ070',      # Lower Back Pain\n",
    "    'DEMO_RIDAGEMN',   # Age (months)\n",
    "    'DEMO_INDFMPIR',   # Poverty Income Ratio\n",
    "    'ALQ_ALQ130',      # Alcohol Consumption\n",
    "    'BMX_BMXBMI',      # BMI\n",
    "    'DEMO_DMDMARTL',   # Marital Status\n",
    "    'DEMO_RIAGENDR',   # Gender\n",
    "    'BIX_BIDFAT',      # Body Fat Mass\n",
    "    'MCQ_MCQ250F',     # Family History HTN/Stroke\n",
    "    'DEMO_DMDHHSIZ'    # Household Size\n",
    "]\n",
    "\n",
    "# EXACT hyperparameters from original model\n",
    "MODEL_PARAMS = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_samples_split': 4,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.25\n",
    "\n",
    "print(f\"\\nüìÇ Data: {DATA_PATH}\")\n",
    "print(f\"üìÇ Output: {OUTPUT_DIR}\")\n",
    "print(f\"\\nüìã Using {len(FEATURES_CLEAN)} clean features\")\n",
    "print(f\"üé≤ Random state: {RANDOM_STATE}\")\n",
    "print(f\"üìä Test size: {TEST_SIZE*100}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. LOADING CLEAN DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"\\n‚úÖ Loaded: {df.shape[0]} samples √ó {df.shape[1]} columns\")\n",
    "\n",
    "# Verify all features are present\n",
    "missing_features = [f for f in FEATURES_CLEAN if f not in df.columns]\n",
    "if missing_features:\n",
    "    print(f\"\\n‚ùå ERROR: Missing features: {missing_features}\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(f\"‚úÖ All {len(FEATURES_CLEAN)} features present\")\n",
    "\n",
    "# Check target\n",
    "if 'target' not in df.columns:\n",
    "    print(\"\\n‚ùå ERROR: 'target' column not found!\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"\\nüéØ Target distribution:\")\n",
    "target_counts = df['target'].value_counts().sort_index()\n",
    "for val, count in target_counts.items():\n",
    "    label = \"Normal\" if val == 0 else \"Panic Disorder\"\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"   {label:15s} (target={val}): {count:5d} ({pct:5.2f}%)\")\n",
    "\n",
    "prevalence = df['target'].mean()\n",
    "print(f\"\\nüìä PD Prevalence: {prevalence*100:.2f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. PREPROCESSING (EXACT SAME AS ORIGINAL)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüîß Imputing missing values with median (same as original)...\")\n",
    "\n",
    "# Create a copy for processing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Get numeric columns (features only, not target)\n",
    "numeric_cols = [col for col in FEATURES_CLEAN if df[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Impute missing values with median (EXACT same as original)\n",
    "for col in numeric_cols:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        df_processed[col] = imputer.fit_transform(df_processed[[col]])\n",
    "        print(f\"   Imputed {col}\")\n",
    "\n",
    "# Check for any remaining missing values\n",
    "missing_after = df_processed[FEATURES_CLEAN].isnull().sum().sum()\n",
    "if missing_after > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Warning: {missing_after} missing values remaining after imputation\")\n",
    "    print(\"   Dropping rows with missing values...\")\n",
    "    df_processed = df_processed.dropna(subset=FEATURES_CLEAN)\n",
    "    print(f\"   New shape: {df_processed.shape}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No missing values after imputation\")\n",
    "\n",
    "# Extract features and target\n",
    "X = df_processed[FEATURES_CLEAN]\n",
    "y = df_processed['target']\n",
    "\n",
    "print(f\"\\n‚úÖ Final dataset:\")\n",
    "print(f\"   X shape: {X.shape}\")\n",
    "print(f\"   y shape: {y.shape}\")\n",
    "print(f\"   Features: {len(FEATURES_CLEAN)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. TRAIN/TEST SPLIT (EXACT SAME AS ORIGINAL)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. TRAIN/TEST SPLIT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=TEST_SIZE, \n",
    "    stratify=y, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Split summary:\")\n",
    "print(f\"   Train: {X_train.shape[0]} samples ({y_train.sum()} PD, {len(y_train)-y_train.sum()} Normal)\")\n",
    "print(f\"   Test:  {X_test.shape[0]} samples ({y_test.sum()} PD, {len(y_test)-y_test.sum()} Normal)\")\n",
    "print(f\"\\n   PD prevalence in train: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"   PD prevalence in test:  {y_test.mean()*100:.2f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. BUILD PIPELINE (EXACT SAME AS ORIGINAL)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. BUILDING MODEL PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüèóÔ∏è  Pipeline components:\")\n",
    "print(\"   1. StandardScaler (normalize features)\")\n",
    "print(\"   2. GradientBoostingClassifier\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Model hyperparameters:\")\n",
    "for param, value in MODEL_PARAMS.items():\n",
    "    print(f\"   {param:20s}: {value}\")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', GradientBoostingClassifier(**MODEL_PARAMS))\n",
    "])\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline created successfully\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. TRAIN MODEL (WITH SAMPLE WEIGHTS - EXACT SAME AS ORIGINAL)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. TRAINING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n‚öñÔ∏è  Computing sample weights (balanced)...\")\n",
    "sample_weights = compute_sample_weight('balanced', y_train)\n",
    "print(f\"   Sample weights computed for {len(sample_weights)} training samples\")\n",
    "\n",
    "print(\"\\nüéØ Training GradientBoostingClassifier...\")\n",
    "print(\"   (This may take 30-60 seconds...)\")\n",
    "\n",
    "pipeline.fit(X_train, y_train, classifier__sample_weight=sample_weights)\n",
    "\n",
    "print(\"\\n‚úÖ Model trained successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. PREDICTIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. GENERATING PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n‚úÖ Predictions generated for test set\")\n",
    "print(f\"   Binary predictions: {len(y_pred)}\")\n",
    "print(f\"   Probability scores: {len(y_proba)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. PERFORMANCE EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. PERFORMANCE EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(\"\\nüìä CLASSIFICATION METRICS:\")\n",
    "print(f\"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"   Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(f\"   F1-Score:  {f1:.4f}\")\n",
    "print(f\"   ROC-AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"\\nüìä CONFUSION MATRIX:\")\n",
    "print(f\"   True Negatives:  {tn}\")\n",
    "print(f\"   False Positives: {fp}\")\n",
    "print(f\"   False Negatives: {fn}\")\n",
    "print(f\"   True Positives:  {tp}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nüìä DETAILED CLASSIFICATION REPORT:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Normal', 'Panic Disorder'], digits=4))\n",
    "\n",
    "# Probability distribution analysis\n",
    "print(\"\\nüìä PROBABILITY DISTRIBUTION:\")\n",
    "print(f\"   Min probability:    {y_proba.min():.4f}\")\n",
    "print(f\"   Max probability:    {y_proba.max():.4f}\")\n",
    "print(f\"   Mean probability:   {y_proba.mean():.4f}\")\n",
    "print(f\"   Median probability: {np.median(y_proba):.4f}\")\n",
    "\n",
    "# Count predictions in different probability ranges\n",
    "print(\"\\nüìä PROBABILITY RANGE DISTRIBUTION:\")\n",
    "ranges = [\n",
    "    (0, 0.01, \"< 1%\"),\n",
    "    (0.01, 0.10, \"1-10%\"),\n",
    "    (0.10, 0.50, \"10-50%\"),\n",
    "    (0.50, 0.90, \"50-90%\"),\n",
    "    (0.90, 0.99, \"90-99%\"),\n",
    "    (0.99, 1.01, \"> 99%\")\n",
    "]\n",
    "\n",
    "for low, high, label in ranges:\n",
    "    count = np.sum((y_proba >= low) & (y_proba < high))\n",
    "    pct = (count / len(y_proba)) * 100\n",
    "    print(f\"   {label:10s}: {count:4d} cases ({pct:5.2f}%)\")\n",
    "\n",
    "# Check for bimodal distribution (like original)\n",
    "borderline = np.sum((y_proba >= 0.10) & (y_proba <= 0.90))\n",
    "extreme = len(y_proba) - borderline\n",
    "print(f\"\\nüéØ DECISION CONFIDENCE:\")\n",
    "print(f\"   Extreme predictions (< 10% or > 90%): {extreme} ({extreme/len(y_proba)*100:.2f}%)\")\n",
    "print(f\"   Borderline (10-90%): {borderline} ({borderline/len(y_proba)*100:.2f}%)\")\n",
    "\n",
    "if borderline == 0:\n",
    "    print(\"\\n   üéâ NO BORDERLINE CASES - Perfect separation maintained!\")\n",
    "elif borderline < 10:\n",
    "    print(f\"\\n   ‚úÖ Very few borderline cases - Strong separation!\")\n",
    "else:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Significant borderline cases - Separation reduced from original\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. CRITICAL ASSESSMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"8. CRITICAL ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüîç COMPARISON WITH ORIGINAL MODEL (12 features with WHQ_WHD050):\")\n",
    "print(\"\\n   Original model metrics:\")\n",
    "print(\"   - Expected accuracy: ~100% (reported in Phase 1)\")\n",
    "print(\"   - Expected borderline: 0 cases\")\n",
    "print(\"   - Expected ROC-AUC: 1.000\")\n",
    "\n",
    "print(f\"\\n   New model metrics (11 clean features):\")\n",
    "print(f\"   - Accuracy:   {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   - Borderline: {borderline} cases\")\n",
    "print(f\"   - ROC-AUC:    {roc_auc:.4f}\")\n",
    "\n",
    "# Determine outcome\n",
    "if accuracy >= 0.99 and borderline <= 1:\n",
    "    print(\"\\n   ‚úÖ RESULT: 100% ACCURACY MAINTAINED!\")\n",
    "    print(\"   üéâ The phenomenon is REAL and not dependent on WHQ_WHD050!\")\n",
    "    print(\"   ‚úì  The 11 clean features alone achieve perfect separation\")\n",
    "    print(\"   ‚úì  Original findings are VALIDATED with clean data\")\n",
    "    outcome = \"SUCCESS\"\n",
    "elif accuracy >= 0.95:\n",
    "    print(\"\\n   ‚ö†Ô∏è  RESULT: Near-perfect accuracy maintained\")\n",
    "    print(f\"   ‚Üí Slight decrease from original (~{(1-accuracy)*100:.1f}% error rate)\")\n",
    "    print(\"   ‚Üí WHQ_WHD050 contributed but was not critical\")\n",
    "    print(\"   ‚Üí Findings remain strong with clean data\")\n",
    "    outcome = \"STRONG\"\n",
    "elif accuracy >= 0.85:\n",
    "    print(\"\\n   ‚ö†Ô∏è  RESULT: Good accuracy but reduced from original\")\n",
    "    print(f\"   ‚Üí Accuracy dropped to {accuracy*100:.1f}%\")\n",
    "    print(\"   ‚Üí WHQ_WHD050 was moderately important\")\n",
    "    print(\"   ‚Üí Findings need re-interpretation\")\n",
    "    outcome = \"MODERATE\"\n",
    "else:\n",
    "    print(\"\\n   ‚ùå RESULT: Significant performance degradation\")\n",
    "    print(f\"   ‚Üí Accuracy dropped to {accuracy*100:.1f}%\")\n",
    "    print(\"   ‚Üí WHQ_WHD050 was critical to original results\")\n",
    "    print(\"   ‚Üí Original 100% accuracy was partially artifact-driven\")\n",
    "    outcome = \"DEGRADED\"\n",
    "\n",
    "# ============================================================================\n",
    "# 9. FEATURE IMPORTANCE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"9. FEATURE IMPORTANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get feature importances from trained model\n",
    "feature_importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "\n",
    "# Create dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': FEATURES_CLEAN,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Top 10 Most Important Features:\")\n",
    "for i, row in importance_df.head(10).iterrows():\n",
    "    print(f\"   {i+1:2d}. {row['Feature']:20s}: {row['Importance']:.4f} ({row['Importance']*100:.2f}%)\")\n",
    "\n",
    "# Check if any single feature is dominant\n",
    "max_importance = importance_df['Importance'].max()\n",
    "if max_importance > 0.5:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  WARNING: Single feature dominance detected ({max_importance*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\n   ‚úÖ No single feature dominance (max = {max_importance*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 10. VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"10. GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Figure 1: ROC Curve\n",
    "print(\"\\nüìä Creating ROC curve...\")\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "ax.plot(fpr, tpr, linewidth=2.5, label=f'Model (AUC = {roc_auc:.4f})', color='#2E86AB')\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Random', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=13, fontweight='bold')\n",
    "ax.set_title('ROC Curve: 11 Clean Features Model\\n(WHQ_WHD050 removed)', \n",
    "             fontsize=15, fontweight='bold', pad=20)\n",
    "ax.legend(loc='lower right', fontsize=12, frameon=True, shadow=True)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add text box with key metrics\n",
    "textstr = f'Accuracy: {accuracy:.4f}\\nPrecision: {precision:.4f}\\nRecall: {recall:.4f}\\nF1-Score: {f1:.4f}'\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "ax.text(0.6, 0.2, textstr, fontsize=11, verticalalignment='top', \n",
    "        bbox=props, family='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "output_fig = OUTPUT_DIR / 'Figure1_ROC_curve_11features.png'\n",
    "plt.savefig(output_fig, dpi=300, bbox_inches='tight')\n",
    "print(f\"   ‚úÖ Saved: {output_fig}\")\n",
    "plt.close()\n",
    "\n",
    "# Figure 2: Probability Distribution\n",
    "print(\"\\nüìä Creating probability distribution plot...\")\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Subplot 1: Overall distribution\n",
    "ax1.hist(y_proba[y_test == 0], bins=50, alpha=0.6, color='#4ECDC4', \n",
    "         label='Normal', edgecolor='black', linewidth=1.2)\n",
    "ax1.hist(y_proba[y_test == 1], bins=30, alpha=0.6, color='#FF6B6B', \n",
    "         label='Panic Disorder', edgecolor='black', linewidth=1.2)\n",
    "ax1.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "ax1.set_xlabel('Predicted Probability', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('Frequency', fontsize=13, fontweight='bold')\n",
    "ax1.set_title('Probability Distribution by True Class', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11, frameon=True, shadow=True)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Subplot 2: Confusion matrix heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2, \n",
    "            xticklabels=['Normal', 'PD'], yticklabels=['Normal', 'PD'],\n",
    "            cbar_kws={'label': 'Count'}, linewidths=2, linecolor='black')\n",
    "ax2.set_xlabel('Predicted', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('True', fontsize=13, fontweight='bold')\n",
    "ax2.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Model Performance Analysis: 11 Clean Features', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "output_fig = OUTPUT_DIR / 'Figure2_probability_distribution_confusion.png'\n",
    "plt.savefig(output_fig, dpi=300, bbox_inches='tight')\n",
    "print(f\"   ‚úÖ Saved: {output_fig}\")\n",
    "plt.close()\n",
    "\n",
    "# Figure 3: Feature Importance\n",
    "print(\"\\nüìä Creating feature importance plot...\")\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(importance_df)))\n",
    "bars = ax.barh(range(len(importance_df)), importance_df['Importance'], \n",
    "               color=colors, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(importance_df.iterrows()):\n",
    "    ax.text(row['Importance'] + 0.005, i, f\"{row['Importance']:.4f}\", \n",
    "            va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_yticks(range(len(importance_df)))\n",
    "ax.set_yticklabels(importance_df['Feature'], fontsize=11)\n",
    "ax.set_xlabel('Importance', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Feature Importance: 11 Clean Features Model\\n(WHQ_WHD050 removed)', \n",
    "             fontsize=15, fontweight='bold', pad=20)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "output_fig = OUTPUT_DIR / 'Figure3_feature_importance_11features.png'\n",
    "plt.savefig(output_fig, dpi=300, bbox_inches='tight')\n",
    "print(f\"   ‚úÖ Saved: {output_fig}\")\n",
    "plt.close()\n",
    "\n",
    "# Figure 4: Comparison scatter plot (predicted vs true)\n",
    "print(\"\\nüìä Creating prediction scatter plot...\")\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Add jitter for visibility\n",
    "np.random.seed(42)\n",
    "jitter_x = np.random.normal(0, 0.02, len(y_test))\n",
    "jitter_y = np.random.normal(0, 0.02, len(y_test))\n",
    "\n",
    "scatter = ax.scatter(y_test + jitter_x, y_proba + jitter_y, \n",
    "                    c=y_proba, cmap='RdYlGn_r', s=50, alpha=0.6, \n",
    "                    edgecolors='black', linewidths=0.5)\n",
    "\n",
    "ax.set_xlabel('True Label (with jitter)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Predicted Probability', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Predicted Probability vs True Label\\n(11 Clean Features)', \n",
    "             fontsize=15, fontweight='bold', pad=20)\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(['Normal', 'Panic Disorder'])\n",
    "ax.axhline(y=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=11)\n",
    "\n",
    "plt.colorbar(scatter, ax=ax, label='Predicted Probability')\n",
    "plt.tight_layout()\n",
    "output_fig = OUTPUT_DIR / 'Figure4_prediction_scatter.png'\n",
    "plt.savefig(output_fig, dpi=300, bbox_inches='tight')\n",
    "print(f\"   ‚úÖ Saved: {output_fig}\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 11. SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"11. SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save model\n",
    "model_path = OUTPUT_DIR / 'panic_model_11features_CLEAN.joblib'\n",
    "joblib.dump(pipeline, model_path)\n",
    "print(f\"\\n‚úÖ Model saved: {model_path}\")\n",
    "\n",
    "# Save predictions\n",
    "results_df = pd.DataFrame({\n",
    "    'True_Label': y_test,\n",
    "    'Predicted_Label': y_pred,\n",
    "    'Predicted_Probability': y_proba\n",
    "})\n",
    "results_path = OUTPUT_DIR / 'predictions_11features.csv'\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"‚úÖ Predictions saved: {results_path}\")\n",
    "\n",
    "# Save feature importance\n",
    "importance_path = OUTPUT_DIR / 'feature_importance_11features.csv'\n",
    "importance_df.to_csv(importance_path, index=False)\n",
    "print(f\"‚úÖ Feature importance saved: {importance_path}\")\n",
    "\n",
    "# Save metrics summary\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC',\n",
    "               'True Negatives', 'False Positives', 'False Negatives', 'True Positives',\n",
    "               'Borderline Cases', 'Extreme Cases'],\n",
    "    'Value': [accuracy, precision, recall, f1, roc_auc,\n",
    "              tn, fp, fn, tp, borderline, extreme]\n",
    "})\n",
    "metrics_path = OUTPUT_DIR / 'metrics_summary_11features.csv'\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "print(f\"‚úÖ Metrics saved: {metrics_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 12. FINAL REPORT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"12. GENERATING FINAL REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "report = f\"\"\"\n",
    "MODEL RE-TRAINING REPORT: 11 CLEAN FEATURES\n",
    "============================================\n",
    "\n",
    "Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "OBJECTIVE\n",
    "---------\n",
    "Test if 100% accuracy is maintained after removing contaminated feature WHQ_WHD050\n",
    "(which contained 99999 coded values creating 96.49% single-variable discrimination).\n",
    "\n",
    "DATASET\n",
    "-------\n",
    "- Source: NHANES_panic_11features_CLEAN.csv\n",
    "- Total samples: {len(df_processed)}\n",
    "- Features: {len(FEATURES_CLEAN)} (WHQ_WHD050 removed)\n",
    "- PD prevalence: {prevalence*100:.2f}%\n",
    "\n",
    "TRAIN/TEST SPLIT\n",
    "----------------\n",
    "- Training: {len(X_train)} samples ({y_train.sum()} PD, {len(y_train)-y_train.sum()} Normal)\n",
    "- Testing: {len(X_test)} samples ({y_test.sum()} PD, {len(y_test)-y_test.sum()} Normal)\n",
    "- Split ratio: {(1-TEST_SIZE)*100:.0f}/{TEST_SIZE*100:.0f}\n",
    "- Random state: {RANDOM_STATE}\n",
    "\n",
    "MODEL CONFIGURATION\n",
    "-------------------\n",
    "Algorithm: Gradient Boosting Classifier\n",
    "Hyperparameters (EXACT same as original):\n",
    "  - n_estimators: {MODEL_PARAMS['n_estimators']}\n",
    "  - max_depth: {MODEL_PARAMS['max_depth']}\n",
    "  - learning_rate: {MODEL_PARAMS['learning_rate']}\n",
    "  - min_samples_leaf: {MODEL_PARAMS['min_samples_leaf']}\n",
    "  - min_samples_split: {MODEL_PARAMS['min_samples_split']}\n",
    "  - random_state: {MODEL_PARAMS['random_state']}\n",
    "\n",
    "Preprocessing:\n",
    "  - StandardScaler (feature normalization)\n",
    "  - SimpleImputer with median strategy\n",
    "  - Balanced sample weights\n",
    "\n",
    "PERFORMANCE METRICS\n",
    "-------------------\n",
    "Accuracy:     {accuracy:.4f} ({accuracy*100:.2f}%)\n",
    "Precision:    {precision:.4f} ({precision*100:.2f}%)\n",
    "Recall:       {recall:.4f} ({recall*100:.2f}%)\n",
    "F1-Score:     {f1:.4f}\n",
    "ROC-AUC:      {roc_auc:.4f}\n",
    "\n",
    "CONFUSION MATRIX\n",
    "----------------\n",
    "                Predicted\n",
    "                Normal    PD\n",
    "Actual Normal     {tn:4d}    {fp:4d}\n",
    "Actual PD         {fn:4d}    {tp:4d}\n",
    "\n",
    "DECISION CONFIDENCE\n",
    "-------------------\n",
    "Extreme predictions (< 10% or > 90%): {extreme} ({extreme/len(y_proba)*100:.2f}%)\n",
    "Borderline (10-90%): {borderline} ({borderline/len(y_proba)*100:.2f}%)\n",
    "\n",
    "TOP 5 MOST IMPORTANT FEATURES\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "for i, row in importance_df.head(5).iterrows():\n",
    "    report += f\"{i+1}. {row['Feature']:20s}: {row['Importance']:.4f} ({row['Importance']*100:.2f}%)\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "\n",
    "CRITICAL ASSESSMENT\n",
    "-------------------\n",
    "Outcome: {outcome}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if outcome == \"SUCCESS\":\n",
    "    report += \"\"\"\n",
    "‚úÖ THE PHENOMENON IS REAL AND VALIDATED!\n",
    "\n",
    "The model achieves 100% accuracy with 11 clean features alone, demonstrating that:\n",
    "1. The perfect separation is NOT dependent on the contaminated WHQ_WHD050\n",
    "2. The 11 remaining features contain sufficient discriminative power\n",
    "3. The original findings are VALID and can be trusted\n",
    "4. The synergistic feature interactions hypothesis is strongly supported\n",
    "\n",
    "NEXT STEPS:\n",
    "- Proceed with all planned analyses (SHAP, UMAP, Decision Trees)\n",
    "- Continue with Paper 3 investigation\n",
    "- Original research conclusions are VALIDATED\n",
    "\"\"\"\n",
    "elif outcome == \"STRONG\":\n",
    "    report += f\"\"\"\n",
    "‚ö†Ô∏è  NEAR-PERFECT PERFORMANCE MAINTAINED\n",
    "\n",
    "The model achieves {accuracy*100:.1f}% accuracy with 11 clean features:\n",
    "1. Performance slightly reduced but remains excellent\n",
    "2. WHQ_WHD050 contributed but was not critical\n",
    "3. Findings remain strong and publishable\n",
    "4. Minor adjustments to paper language needed\n",
    "\n",
    "NEXT STEPS:\n",
    "- Proceed with analyses but note the small performance difference\n",
    "- Update paper to report {accuracy*100:.1f}% accuracy with clean data\n",
    "- Emphasize that contaminated feature was detected and removed\n",
    "\"\"\"\n",
    "elif outcome == \"MODERATE\":\n",
    "    report += f\"\"\"\n",
    "‚ö†Ô∏è  MODERATE PERFORMANCE MAINTAINED\n",
    "\n",
    "The model achieves {accuracy*100:.1f}% accuracy with 11 clean features:\n",
    "1. Significant reduction from original 100%\n",
    "2. WHQ_WHD050 was moderately important\n",
    "3. Findings need re-interpretation\n",
    "4. Paper focus should shift from \"perfect\" to \"high accuracy\"\n",
    "\n",
    "NEXT STEPS:\n",
    "- Continue analyses but adjust expectations\n",
    "- Re-frame paper around {accuracy*100:.1f}% accuracy\n",
    "- Investigate why WHQ_WHD050 was important (beyond the 99999 codes)\n",
    "- Consider ensemble with other features\n",
    "\"\"\"\n",
    "else:\n",
    "    report += f\"\"\"\n",
    "‚ùå SIGNIFICANT PERFORMANCE DEGRADATION\n",
    "\n",
    "The model achieves only {accuracy*100:.1f}% accuracy with 11 clean features:\n",
    "1. Major reduction from original 100%\n",
    "2. WHQ_WHD050 was critical to original results\n",
    "3. Original 100% accuracy was substantially artifact-driven\n",
    "4. Project requires major re-evaluation\n",
    "\n",
    "NEXT STEPS:\n",
    "- Re-evaluate whether to continue with current approach\n",
    "- Consider alternative modeling strategies\n",
    "- May need to pivot Paper 3 focus to data quality issues\n",
    "- Thoroughly investigate remaining features for other artifacts\n",
    "\"\"\"\n",
    "\n",
    "report += f\"\"\"\n",
    "\n",
    "TECHNICAL DETAILS\n",
    "-----------------\n",
    "All files saved to: {OUTPUT_DIR}\n",
    "\n",
    "Generated files:\n",
    "- panic_model_11features_CLEAN.joblib (trained model)\n",
    "- predictions_11features.csv (test set predictions)\n",
    "- feature_importance_11features.csv (feature rankings)\n",
    "- metrics_summary_11features.csv (performance metrics)\n",
    "- Figure1_ROC_curve_11features.png\n",
    "- Figure2_probability_distribution_confusion.png\n",
    "- Figure3_feature_importance_11features.png\n",
    "- Figure4_prediction_scatter.png\n",
    "\n",
    "REPRODUCIBILITY\n",
    "---------------\n",
    "Random state: {RANDOM_STATE} (fixed for reproducibility)\n",
    "All hyperparameters identical to original model\n",
    "Same preprocessing pipeline\n",
    "Same train/test split strategy\n",
    "\n",
    "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "\n",
    "report_path = OUTPUT_DIR / 'MODEL_RETRAIN_REPORT.txt'\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report)\n",
    "print(f\"\\n‚úÖ Report saved: {report_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ MODEL RE-TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüéØ FINAL RESULT: {outcome}\")\n",
    "print(f\"\\nüìä Key Metrics:\")\n",
    "print(f\"   Accuracy:  {accuracy*100:.2f}%\")\n",
    "print(f\"   ROC-AUC:   {roc_auc:.4f}\")\n",
    "print(f\"   Borderline: {borderline} cases\")\n",
    "\n",
    "if outcome == \"SUCCESS\":\n",
    "    print(\"\\nüéâ THE 100% ACCURACY IS REAL!\")\n",
    "    print(\"   ‚úì Findings validated with clean data\")\n",
    "    print(\"   ‚úì Continue with Paper 3 investigations\")\n",
    "    print(\"   ‚úì WHQ_WHD050 was not critical\")\n",
    "elif outcome == \"STRONG\":\n",
    "    print(\"\\n‚úÖ Near-perfect accuracy maintained!\")\n",
    "    print(\"   ‚úì Findings remain strong\")\n",
    "    print(\"   ‚úì Minor adjustments to paper needed\")\n",
    "elif outcome == \"MODERATE\":\n",
    "    print(\"\\n‚ö†Ô∏è  Accuracy reduced but still good\")\n",
    "    print(\"   ‚Üí Re-frame paper expectations\")\n",
    "    print(\"   ‚Üí Continue with adjusted approach\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Significant accuracy drop\")\n",
    "    print(\"   ‚Üí Original results partly artifact-driven\")\n",
    "    print(\"   ‚Üí Major re-evaluation needed\")\n",
    "\n",
    "print(f\"\\nüìÇ All results saved to: {OUTPUT_DIR}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
